"""å®æ—¶è¯­éŸ³æœåŠ¡æ¨¡å—
ä¸“ä¸ºChatGPTé£æ ¼çš„å®æ—¶è¯­éŸ³äº¤äº’ä¼˜åŒ–
"""

import asyncio
import logging
import json
import time
from typing import Dict, Any, Optional, AsyncGenerator, List, Callable
from dataclasses import dataclass
from enum import Enum
import numpy as np
from core.speech import SpeechService
from .config import config
from core.whisper_model_manager import get_model_manager

logger = logging.getLogger(__name__)

class VoiceActivityState(Enum):
    """è¯­éŸ³æ´»åŠ¨çŠ¶æ€"""
    SILENCE = "silence"
    SPEECH = "speech"
    PROCESSING = "processing"
    SPEAKING = "speaking"

@dataclass
class AudioChunk:
    """éŸ³é¢‘å—æ•°æ®"""
    data: bytes
    timestamp: float
    sample_rate: int = 16000
    channels: int = 1
    
@dataclass
class VoiceActivityResult:
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç»“æœ"""
    is_speech: bool
    confidence: float
    energy: float
    timestamp: float

@dataclass
class TranscriptionResult:
    """è½¬å½•ç»“æœ"""
    text: str
    confidence: float
    is_final: bool
    timestamp: float
    duration: float
    word_count: int

class RealTimeSpeechService:
    """å®æ—¶è¯­éŸ³æœåŠ¡"""
    
    def __init__(self):
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        self._check_model_status()
        
        self.speech_service = SpeechService()
        self.vad_threshold = 0.005  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹é˜ˆå€¼ï¼ˆé™ä½ä»¥æé«˜çµæ•åº¦ï¼‰
        self.silence_timeout = 2.0  # é™é»˜è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.min_speech_duration = 0.5  # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´
        self.max_speech_duration = 30.0  # æœ€å¤§è¯­éŸ³æŒç»­æ—¶é—´
        
        # çŠ¶æ€ç®¡ç†
        self.current_state = VoiceActivityState.SILENCE
        self.speech_start_time = None
        self.last_speech_time = None
        self.accumulated_audio = bytearray()
        
        # å›è°ƒå‡½æ•°
        self.on_speech_start: Optional[Callable] = None
        self.on_speech_end: Optional[Callable] = None
        self.on_transcription: Optional[Callable[[TranscriptionResult], None]] = None
        self.on_state_change: Optional[Callable[[VoiceActivityState], None]] = None
        
        logger.info("RealTimeSpeechService åˆå§‹åŒ–å®Œæˆ")
    
    def _check_model_status(self):
        """æ£€æŸ¥å¹¶æŠ¥å‘Šæ¨¡å‹çŠ¶æ€"""
        try:
            model_manager = get_model_manager()
            local_whisper_config = config.get_local_whisper_config()
            model_size = local_whisper_config.get("model_size", "medium")
            
            model_info = model_manager.get_model_info(model_size)
            if model_info and model_info.get("installed"):
                logger.info(f"âœ… Whisperæ¨¡å‹ {model_size} å·²å°±ç»ª: {model_info.get('path')}")
            else:
                logger.warning(f"âš ï¸ Whisperæ¨¡å‹ {model_size} æœªå®‰è£…")
                logger.info("ğŸ’¡ æç¤º: è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹:")
                logger.info(f"   python scripts/download_faster_whisper.py {model_size}")
        except Exception as e:
            logger.warning(f"æ£€æŸ¥æ¨¡å‹çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    def set_callbacks(
        self,
        on_speech_start: Optional[Callable] = None,
        on_speech_end: Optional[Callable] = None,
        on_transcription: Optional[Callable[[TranscriptionResult], None]] = None,
        on_state_change: Optional[Callable[[VoiceActivityState], None]] = None
    ):
        """è®¾ç½®å›è°ƒå‡½æ•°"""
        self.on_speech_start = on_speech_start
        self.on_speech_end = on_speech_end
        self.on_transcription = on_transcription
        self.on_state_change = on_state_change
    
    def _change_state(self, new_state: VoiceActivityState):
        """æ”¹å˜çŠ¶æ€å¹¶è§¦å‘å›è°ƒ"""
        if self.current_state != new_state:
            old_state = self.current_state
            self.current_state = new_state
            logger.debug(f"çŠ¶æ€å˜åŒ–: {old_state.value} -> {new_state.value}")
            
            if self.on_state_change:
                try:
                    self.on_state_change(new_state)
                except Exception as e:
                    logger.error(f"çŠ¶æ€å˜åŒ–å›è°ƒå¤±è´¥: {e}")
    
    def _detect_voice_activity(self, audio_chunk: AudioChunk) -> VoiceActivityResult:
        """ç®€å•çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        try:
            # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œåˆ†æ
            # å‡è®¾æ˜¯16ä½PCMæ•°æ®
            audio_array = np.frombuffer(audio_chunk.data, dtype=np.int16)
            
            # è®¡ç®—èƒ½é‡
            energy = np.mean(np.abs(audio_array.astype(np.float32))) / 32768.0
            
            # ç®€å•çš„é˜ˆå€¼æ£€æµ‹
            is_speech = energy > self.vad_threshold
            confidence = min(energy / self.vad_threshold, 1.0) if is_speech else 0.0
            
            return VoiceActivityResult(
                is_speech=is_speech,
                confidence=confidence,
                energy=energy,
                timestamp=audio_chunk.timestamp
            )
        except Exception as e:
            logger.error(f"è¯­éŸ³æ´»åŠ¨æ£€æµ‹å¤±è´¥: {e}")
            return VoiceActivityResult(
                is_speech=False,
                confidence=0.0,
                energy=0.0,
                timestamp=audio_chunk.timestamp
            )
    
    async def process_audio_chunk(self, audio_chunk: AudioChunk) -> Optional[TranscriptionResult]:
        """å¤„ç†éŸ³é¢‘å—"""
        current_time = time.time()
        vad_result = self._detect_voice_activity(audio_chunk)
        
        # çŠ¶æ€æœºé€»è¾‘
        if self.current_state == VoiceActivityState.SILENCE:
            if vad_result.is_speech:
                # å¼€å§‹æ£€æµ‹åˆ°è¯­éŸ³
                self.speech_start_time = current_time
                self.last_speech_time = current_time
                self.accumulated_audio = bytearray(audio_chunk.data)
                self._change_state(VoiceActivityState.SPEECH)
                
                if self.on_speech_start:
                    try:
                        await self.on_speech_start()
                    except Exception as e:
                        logger.error(f"è¯­éŸ³å¼€å§‹å›è°ƒå¤±è´¥: {e}")
        
        elif self.current_state == VoiceActivityState.SPEECH:
            if vad_result.is_speech:
                # ç»§ç»­æ£€æµ‹åˆ°è¯­éŸ³
                self.last_speech_time = current_time
                self.accumulated_audio.extend(audio_chunk.data)
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¯­éŸ³æŒç»­æ—¶é—´
                if current_time - self.speech_start_time > self.max_speech_duration:
                    logger.warning("è¯­éŸ³æŒç»­æ—¶é—´è¿‡é•¿ï¼Œå¼ºåˆ¶ç»“æŸ")
                    return await self._finalize_speech()
            
            else:
                # æ£€æµ‹åˆ°é™é»˜
                self.accumulated_audio.extend(audio_chunk.data)
                
                # æ£€æŸ¥é™é»˜æŒç»­æ—¶é—´
                if current_time - self.last_speech_time > self.silence_timeout:
                    # é™é»˜è¶…æ—¶ï¼Œç»“æŸè¯­éŸ³
                    speech_duration = current_time - self.speech_start_time
                    if speech_duration >= self.min_speech_duration:
                        return await self._finalize_speech()
                    else:
                        # è¯­éŸ³å¤ªçŸ­ï¼Œå¿½ç•¥
                        logger.debug(f"è¯­éŸ³æŒç»­æ—¶é—´å¤ªçŸ­ ({speech_duration:.2f}s)ï¼Œå¿½ç•¥")
                        self._reset_speech_state()
        
        return None
    
    async def _finalize_speech(self) -> Optional[TranscriptionResult]:
        """å®Œæˆè¯­éŸ³å¤„ç†"""
        if not self.accumulated_audio:
            self._reset_speech_state()
            return None
        
        self._change_state(VoiceActivityState.PROCESSING)
        
        try:
            # è§¦å‘è¯­éŸ³ç»“æŸå›è°ƒ
            if self.on_speech_end:
                try:
                    await self.on_speech_end()
                except Exception as e:
                    logger.error(f"è¯­éŸ³ç»“æŸå›è°ƒå¤±è´¥: {e}")
            
            # è¿›è¡Œè¯­éŸ³è¯†åˆ«
            audio_data = bytes(self.accumulated_audio)
            speech_duration = time.time() - self.speech_start_time
            
            logger.info(f"å¼€å§‹è½¬å½• {len(audio_data)} å­—èŠ‚éŸ³é¢‘ï¼ŒæŒç»­æ—¶é—´ {speech_duration:.2f}s")
            
            transcription_data = await self.speech_service.speech_to_text(
                audio_data=audio_data,
                language="zh"
            )
            
            # ä¿®å¤ï¼šspeech_to_textè¿”å›çš„æ•°æ®ç»“æ„ä¸åŒ…å«successå­—æ®µ
            if transcription_data and transcription_data.get("text"):
                text = transcription_data["text"].strip()
                confidence = transcription_data.get("confidence", 0.95)
                word_count = len(text.split()) if text else 0
                
                result = TranscriptionResult(
                    text=text,
                    confidence=confidence,
                    is_final=True,
                    timestamp=time.time(),
                    duration=speech_duration,
                    word_count=word_count
                )
                
                logger.info(f"è½¬å½•æˆåŠŸ: '{text}' (ç½®ä¿¡åº¦: {confidence:.2f})")
                
                # è§¦å‘è½¬å½•å›è°ƒ
                if self.on_transcription:
                    try:
                        await self.on_transcription(result)
                    except Exception as e:
                        logger.error(f"è½¬å½•å›è°ƒå¤±è´¥: {e}")
                
                self._reset_speech_state()
                return result
            else:
                logger.warning("è½¬å½•å¤±è´¥æˆ–ç»“æœä¸ºç©º")
                self._reset_speech_state()
                return None
                
        except Exception as e:
            logger.error(f"è¯­éŸ³å¤„ç†å¤±è´¥: {e}")
            self._reset_speech_state()
            return None
    
    def _reset_speech_state(self):
        """é‡ç½®è¯­éŸ³çŠ¶æ€"""
        self.speech_start_time = None
        self.last_speech_time = None
        self.accumulated_audio = bytearray()
        self._change_state(VoiceActivityState.SILENCE)
    
    async def synthesize_speech(
        self,
        text: str,
        voice: str = "nova",
        speed: float = 1.0,
        interrupt_current: bool = True
    ) -> bytes:
        """åˆæˆè¯­éŸ³"""
        if interrupt_current:
            self._change_state(VoiceActivityState.SPEAKING)
        
        try:
            audio_data = await self.speech_service.text_to_speech(
                text=text,
                voice=voice,
                speed=speed
            )
            
            logger.info(f"è¯­éŸ³åˆæˆæˆåŠŸ: '{text[:50]}...' ({len(audio_data)} å­—èŠ‚)")
            return audio_data
            
        except Exception as e:
            logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
            raise
        finally:
            if interrupt_current:
                self._change_state(VoiceActivityState.SILENCE)
    
    async def stream_synthesize_speech(
        self,
        text: str,
        voice: str = "nova",
        speed: float = 1.0
    ) -> AsyncGenerator[bytes, None]:
        """æµå¼è¯­éŸ³åˆæˆ"""
        self._change_state(VoiceActivityState.SPEAKING)
        
        try:
            async for chunk in self.speech_service.stream_text_to_speech(
                text=text,
                voice=voice,
                speed=speed
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"æµå¼è¯­éŸ³åˆæˆå¤±è´¥: {e}")
            raise
        finally:
            self._change_state(VoiceActivityState.SILENCE)
    
    def get_current_state(self) -> VoiceActivityState:
        """è·å–å½“å‰çŠ¶æ€"""
        return self.current_state
    
    def is_listening(self) -> bool:
        """æ˜¯å¦æ­£åœ¨ç›‘å¬"""
        return self.current_state in [VoiceActivityState.SILENCE, VoiceActivityState.SPEECH]
    
    def is_processing(self) -> bool:
        """æ˜¯å¦æ­£åœ¨å¤„ç†"""
        return self.current_state == VoiceActivityState.PROCESSING
    
    def is_speaking(self) -> bool:
        """æ˜¯å¦æ­£åœ¨è¯´è¯"""
        return self.current_state == VoiceActivityState.SPEAKING
    
    def configure(
        self,
        vad_threshold: Optional[float] = None,
        silence_timeout: Optional[float] = None,
        min_speech_duration: Optional[float] = None,
        max_speech_duration: Optional[float] = None
    ):
        """é…ç½®å‚æ•°"""
        if vad_threshold is not None:
            self.vad_threshold = max(0.001, min(1.0, vad_threshold))
        if silence_timeout is not None:
            self.silence_timeout = max(0.5, min(10.0, silence_timeout))
        if min_speech_duration is not None:
            self.min_speech_duration = max(0.1, min(5.0, min_speech_duration))
        if max_speech_duration is not None:
            self.max_speech_duration = max(5.0, min(60.0, max_speech_duration))
        
        logger.info(f"é…ç½®æ›´æ–°: VADé˜ˆå€¼={self.vad_threshold}, é™é»˜è¶…æ—¶={self.silence_timeout}s")

# å…¨å±€å®ä¾‹
realtime_speech_service = RealTimeSpeechService()