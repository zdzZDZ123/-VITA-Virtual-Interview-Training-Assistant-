"""
å¤šæ¨¡æ€é¢è¯•è¯„ä¼°æ¨¡å—
ä½¿ç”¨Doubao-Seed-1.6æ¨¡å‹åˆ†æç”¨æˆ·çš„è¡¨æƒ…ã€è¯­æ°”ã€è‚¢ä½“è¯­è¨€å’Œå›ç­”å†…å®¹
æä¾›å…¨æ–¹ä½çš„é¢è¯•è¡¨ç°è¯„ä¼°
"""

import asyncio
import logging
import json
import time
import base64
import cv2
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import io
from PIL import Image

from .config import VITAConfig
from .qwen_llama_client import get_client_manager

logger = logging.getLogger(__name__)

class EmotionState(Enum):
    """æƒ…ç»ªçŠ¶æ€"""
    CONFIDENT = "confident"      # è‡ªä¿¡
    NERVOUS = "nervous"          # ç´§å¼ 
    CALM = "calm"               # å†·é™
    EXCITED = "excited"         # å…´å¥‹
    CONFUSED = "confused"       # å›°æƒ‘
    FOCUSED = "focused"         # ä¸“æ³¨
    STRESSED = "stressed"       # å‹åŠ›å¤§

class EngagementLevel(Enum):
    """å‚ä¸åº¦çº§åˆ«"""
    HIGH = "high"               # é«˜åº¦å‚ä¸
    MEDIUM = "medium"           # ä¸­ç­‰å‚ä¸
    LOW = "low"                 # ä½å‚ä¸
    DISTRACTED = "distracted"   # åˆ†å¿ƒ

@dataclass
class FacialAnalysis:
    """é¢éƒ¨è¡¨æƒ…åˆ†æç»“æœ"""
    emotion: EmotionState
    confidence: float           # ç½®ä¿¡åº¦ 0-1
    eye_contact: float         # çœ¼ç¥äº¤æµç¨‹åº¦ 0-1
    smile_intensity: float     # å¾®ç¬‘å¼ºåº¦ 0-1
    attention_level: float     # æ³¨æ„åŠ›æ°´å¹³ 0-1
    timestamp: float

@dataclass
class VoiceAnalysis:
    """è¯­éŸ³åˆ†æç»“æœ"""
    tone_confidence: float     # è¯­è°ƒè‡ªä¿¡åº¦ 0-1
    speech_pace: float         # è¯­é€Ÿ (words per minute)
    volume_level: float        # éŸ³é‡æ°´å¹³ 0-1
    clarity: float            # æ¸…æ™°åº¦ 0-1
    emotional_tone: str       # æƒ…æ„Ÿè¯­è°ƒ
    pause_frequency: float    # åœé¡¿é¢‘ç‡
    timestamp: float

@dataclass
class ContentAnalysis:
    """å†…å®¹åˆ†æç»“æœ"""
    relevance_score: float    # ç›¸å…³æ€§å¾—åˆ† 0-1
    completeness: float       # å®Œæ•´æ€§ 0-1
    technical_accuracy: float # æŠ€æœ¯å‡†ç¡®æ€§ 0-1
    communication_clarity: float # è¡¨è¾¾æ¸…æ™°åº¦ 0-1
    keywords_coverage: List[str] # å…³é”®è¯è¦†ç›–
    timestamp: float

@dataclass
class MultimodalAssessment:
    """ç»¼åˆå¤šæ¨¡æ€è¯„ä¼°ç»“æœ"""
    overall_score: float      # æ€»ä½“å¾—åˆ† 0-1
    facial_analysis: FacialAnalysis
    voice_analysis: VoiceAnalysis
    content_analysis: ContentAnalysis
    engagement_level: EngagementLevel
    recommendations: List[str] # æ”¹è¿›å»ºè®®
    strengths: List[str]      # ä¼˜åŠ¿ç‚¹
    areas_for_improvement: List[str] # éœ€è¦æ”¹è¿›çš„åœ°æ–¹
    timestamp: float

class MultimodalInterviewEvaluator:
    """å¤šæ¨¡æ€é¢è¯•è¯„ä¼°å™¨
    
    ä½¿ç”¨Doubao-Seed-1.6æ¨¡å‹è¿›è¡Œå…¨æ–¹ä½é¢è¯•è¯„ä¼°
    """
    
    def __init__(self):
        self.model_name = "Doubao-Seed-1.6"
        self.client_manager = get_client_manager()
        
        # è¯„ä¼°å†å²
        self.assessment_history: List[MultimodalAssessment] = []
        
        # å½“å‰é¢è¯•ä¼šè¯é…ç½®
        self.current_session = {
            "start_time": 0,
            "question_count": 0,
            "current_question": "",
            "expected_keywords": [],
            "position_type": "general"  # èŒä½ç±»å‹
        }
        
        logger.info("ğŸ­ å¤šæ¨¡æ€é¢è¯•è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_interview_session(self, position_type: str = "general", expected_skills: List[str] = None):
        """å¼€å§‹é¢è¯•ä¼šè¯"""
        self.current_session = {
            "start_time": time.time(),
            "question_count": 0,
            "current_question": "",
            "expected_keywords": expected_skills or [],
            "position_type": position_type
        }
        self.assessment_history.clear()
        logger.info(f"ğŸ¬ å¼€å§‹é¢è¯•ä¼šè¯: {position_type}")
    
    async def analyze_facial_expression(self, image_data: bytes) -> FacialAnalysis:
        """åˆ†æé¢éƒ¨è¡¨æƒ…"""
        try:
            # å°†å›¾åƒç¼–ç ä¸ºbase64
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # æ„å»ºå¤šæ¨¡æ€åˆ†æprompt
            prompt = """
ä½œä¸ºä¸“ä¸šçš„é¢è¯•å®˜ï¼Œè¯·åˆ†æè¿™å¼ é¢è¯•è€…çš„ç…§ç‰‡ï¼Œè¯„ä¼°ä»¥ä¸‹æ–¹é¢ï¼š

1. æƒ…ç»ªçŠ¶æ€ (confident/nervous/calm/excited/confused/focused/stressed)
2. çœ¼ç¥äº¤æµç¨‹åº¦ (0-1, 1è¡¨ç¤ºå®Œå…¨ç›´è§†)
3. å¾®ç¬‘å¼ºåº¦ (0-1, 1è¡¨ç¤ºè‡ªç„¶å¾®ç¬‘)
4. æ³¨æ„åŠ›æ°´å¹³ (0-1, 1è¡¨ç¤ºé«˜åº¦ä¸“æ³¨)
5. æ•´ä½“ç½®ä¿¡åº¦ (0-1)

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
  "emotion": "æƒ…ç»ªçŠ¶æ€",
  "confidence": 0.8,
  "eye_contact": 0.7,
  "smile_intensity": 0.6,
  "attention_level": 0.9
}
"""

            # è·å–å®¢æˆ·ç«¯å¹¶å‘é€è¯·æ±‚
            client = await self.client_manager.get_healthy_client(provider_type="doubao")
            if not client:
                raise Exception("è±†åŒ…å®¢æˆ·ç«¯ä¸å¯ç”¨")
            
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            # è§£æå“åº”
            result_text = response.choices[0].message.content
            result_json = json.loads(result_text)
            
            return FacialAnalysis(
                emotion=EmotionState(result_json.get("emotion", "calm")),
                confidence=result_json.get("confidence", 0.5),
                eye_contact=result_json.get("eye_contact", 0.5),
                smile_intensity=result_json.get("smile_intensity", 0.5),
                attention_level=result_json.get("attention_level", 0.5),
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"âŒ é¢éƒ¨è¡¨æƒ…åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return FacialAnalysis(
                emotion=EmotionState.CALM,
                confidence=0.5,
                eye_contact=0.5,
                smile_intensity=0.5,
                attention_level=0.5,
                timestamp=time.time()
            )
    
    async def analyze_voice_characteristics(self, audio_data: bytes, transcript: str) -> VoiceAnalysis:
        """åˆ†æè¯­éŸ³ç‰¹å¾"""
        try:
            # æ„å»ºè¯­éŸ³åˆ†æprompt
            prompt = f"""
ä½œä¸ºè¯­éŸ³åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹é¢è¯•å›ç­”çš„è¯­éŸ³ç‰¹å¾ï¼š

è½¬å½•æ–‡æœ¬: "{transcript}"

è¯·è¯„ä¼°ä»¥ä¸‹è¯­éŸ³ç‰¹å¾ï¼š
1. è¯­è°ƒè‡ªä¿¡åº¦ (0-1, 1è¡¨ç¤ºéå¸¸è‡ªä¿¡)
2. è¯­é€Ÿé€‚ä¸­ç¨‹åº¦ (ä»¥words per minuteä¼°ç®—)
3. éŸ³é‡é€‚ä¸­ç¨‹åº¦ (0-1, 0.5ä¸ºé€‚ä¸­)
4. è¡¨è¾¾æ¸…æ™°åº¦ (0-1, 1è¡¨ç¤ºéå¸¸æ¸…æ™°)
5. æƒ…æ„Ÿè¯­è°ƒæè¿°
6. åœé¡¿é¢‘ç‡ (0-1, 0.3ä¸ºé€‚ä¸­)

åŸºäºæ–‡æœ¬å†…å®¹æ¨æ–­è¿™äº›ç‰¹å¾ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "tone_confidence": 0.8,
  "speech_pace": 150,
  "volume_level": 0.6,
  "clarity": 0.9,
  "emotional_tone": "è‡ªä¿¡ä¸”ä¸“ä¸š",
  "pause_frequency": 0.3
}}
"""

            client = await self.client_manager.get_healthy_client(provider_type="doubao")
            if not client:
                raise Exception("è±†åŒ…å®¢æˆ·ç«¯ä¸å¯ç”¨")
            
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=400,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content
            result_json = json.loads(result_text)
            
            return VoiceAnalysis(
                tone_confidence=result_json.get("tone_confidence", 0.5),
                speech_pace=result_json.get("speech_pace", 150),
                volume_level=result_json.get("volume_level", 0.5),
                clarity=result_json.get("clarity", 0.5),
                emotional_tone=result_json.get("emotional_tone", "å¹³é™"),
                pause_frequency=result_json.get("pause_frequency", 0.3),
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return VoiceAnalysis(
                tone_confidence=0.5,
                speech_pace=150,
                volume_level=0.5,
                clarity=0.5,
                emotional_tone="å¹³é™",
                pause_frequency=0.3,
                timestamp=time.time()
            )
    
    async def analyze_content_quality(self, question: str, answer: str) -> ContentAnalysis:
        """åˆ†æå›ç­”å†…å®¹è´¨é‡"""
        try:
            prompt = f"""
ä½œä¸ºé¢è¯•ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹é¢è¯•é—®ç­”çš„è´¨é‡ï¼š

é—®é¢˜: "{question}"
å›ç­”: "{answer}"

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆ0-1åˆ†ï¼‰ï¼š
1. ç›¸å…³æ€§å¾—åˆ† - å›ç­”æ˜¯å¦ç›´æ¥ç›¸å…³
2. å®Œæ•´æ€§ - å›ç­”æ˜¯å¦å®Œæ•´å…¨é¢
3. æŠ€æœ¯å‡†ç¡®æ€§ - æŠ€æœ¯å†…å®¹æ˜¯å¦å‡†ç¡®
4. è¡¨è¾¾æ¸…æ™°åº¦ - é€»è¾‘æ˜¯å¦æ¸…æ™°
5. å…³é”®è¯è¦†ç›– - æåˆ°äº†å“ªäº›é‡è¦å…³é”®è¯

ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "relevance_score": 0.8,
  "completeness": 0.7,
  "technical_accuracy": 0.9,
  "communication_clarity": 0.8,
  "keywords_coverage": ["å…³é”®è¯1", "å…³é”®è¯2"]
}}
"""

            client = await self.client_manager.get_healthy_client(provider_type="doubao")
            if not client:
                raise Exception("è±†åŒ…å®¢æˆ·ç«¯ä¸å¯ç”¨")
            
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content
            result_json = json.loads(result_text)
            
            return ContentAnalysis(
                relevance_score=result_json.get("relevance_score", 0.5),
                completeness=result_json.get("completeness", 0.5),
                technical_accuracy=result_json.get("technical_accuracy", 0.5),
                communication_clarity=result_json.get("communication_clarity", 0.5),
                keywords_coverage=result_json.get("keywords_coverage", []),
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"âŒ å†…å®¹è´¨é‡åˆ†æå¤±è´¥: {e}")
            return ContentAnalysis(
                relevance_score=0.5,
                completeness=0.5,
                technical_accuracy=0.5,
                communication_clarity=0.5,
                keywords_coverage=[],
                timestamp=time.time()
            )
    
    async def comprehensive_assessment(
        self,
        question: str,
        answer: str,
        image_data: Optional[bytes] = None,
        audio_data: Optional[bytes] = None
    ) -> MultimodalAssessment:
        """ç»¼åˆå¤šæ¨¡æ€è¯„ä¼°"""
        try:
            # å¹¶è¡Œæ‰§è¡Œå„é¡¹åˆ†æ
            tasks = []
            
            # å†…å®¹åˆ†æï¼ˆå¿…é¡»ï¼‰
            content_task = self.analyze_content_quality(question, answer)
            tasks.append(content_task)
            
            # é¢éƒ¨è¡¨æƒ…åˆ†æï¼ˆå¦‚æœæœ‰å›¾åƒï¼‰
            if image_data:
                facial_task = self.analyze_facial_expression(image_data)
                tasks.append(facial_task)
            else:
                facial_task = None
            
            # è¯­éŸ³åˆ†æï¼ˆå¦‚æœæœ‰éŸ³é¢‘ï¼‰
            if audio_data:
                voice_task = self.analyze_voice_characteristics(audio_data, answer)
                tasks.append(voice_task)
            else:
                voice_task = None
            
            # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            content_analysis = results[0] if isinstance(results[0], ContentAnalysis) else ContentAnalysis(
                relevance_score=0.5, completeness=0.5, technical_accuracy=0.5,
                communication_clarity=0.5, keywords_coverage=[], timestamp=time.time()
            )
            
            facial_analysis = None
            voice_analysis = None
            
            result_idx = 1
            if facial_task:
                facial_analysis = results[result_idx] if isinstance(results[result_idx], FacialAnalysis) else FacialAnalysis(
                    emotion=EmotionState.CALM, confidence=0.5, eye_contact=0.5,
                    smile_intensity=0.5, attention_level=0.5, timestamp=time.time()
                )
                result_idx += 1
            
            if voice_task:
                voice_analysis = results[result_idx] if isinstance(results[result_idx], VoiceAnalysis) else VoiceAnalysis(
                    tone_confidence=0.5, speech_pace=150, volume_level=0.5,
                    clarity=0.5, emotional_tone="å¹³é™", pause_frequency=0.3, timestamp=time.time()
                )
            
            # å¦‚æœæ²¡æœ‰é¢éƒ¨æˆ–è¯­éŸ³åˆ†æï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not facial_analysis:
                facial_analysis = FacialAnalysis(
                    emotion=EmotionState.CALM, confidence=0.5, eye_contact=0.5,
                    smile_intensity=0.5, attention_level=0.5, timestamp=time.time()
                )
            
            if not voice_analysis:
                voice_analysis = VoiceAnalysis(
                    tone_confidence=0.5, speech_pace=150, volume_level=0.5,
                    clarity=0.5, emotional_tone="å¹³é™", pause_frequency=0.3, timestamp=time.time()
                )
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            overall_score = self._calculate_overall_score(
                content_analysis, facial_analysis, voice_analysis
            )
            
            # ç¡®å®šå‚ä¸åº¦
            engagement_level = self._determine_engagement_level(
                facial_analysis, voice_analysis, content_analysis
            )
            
            # ç”Ÿæˆå»ºè®®å’Œè¯„ä»·
            recommendations, strengths, improvements = await self._generate_feedback(
                content_analysis, facial_analysis, voice_analysis, overall_score
            )
            
            assessment = MultimodalAssessment(
                overall_score=overall_score,
                facial_analysis=facial_analysis,
                voice_analysis=voice_analysis,
                content_analysis=content_analysis,
                engagement_level=engagement_level,
                recommendations=recommendations,
                strengths=strengths,
                areas_for_improvement=improvements,
                timestamp=time.time()
            )
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.assessment_history.append(assessment)
            self.current_session["question_count"] += 1
            
            logger.info(f"âœ… å¤šæ¨¡æ€è¯„ä¼°å®Œæˆï¼Œç»¼åˆå¾—åˆ†: {overall_score:.2f}")
            return assessment
            
        except Exception as e:
            logger.error(f"âŒ ç»¼åˆè¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¯„ä¼°
            return self._create_default_assessment()
    
    def _calculate_overall_score(
        self,
        content: ContentAnalysis,
        facial: FacialAnalysis,
        voice: VoiceAnalysis
    ) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        # æƒé‡åˆ†é…ï¼šå†…å®¹50%ï¼Œé¢éƒ¨è¡¨æƒ…25%ï¼Œè¯­éŸ³25%
        content_score = (
            content.relevance_score * 0.3 +
            content.completeness * 0.3 +
            content.technical_accuracy * 0.2 +
            content.communication_clarity * 0.2
        )
        
        facial_score = (
            facial.confidence * 0.4 +
            facial.eye_contact * 0.3 +
            facial.attention_level * 0.3
        )
        
        voice_score = (
            voice.tone_confidence * 0.4 +
            voice.clarity * 0.3 +
            (1 - abs(voice.volume_level - 0.5) * 2) * 0.3  # éŸ³é‡é€‚ä¸­æ€§
        )
        
        overall = content_score * 0.5 + facial_score * 0.25 + voice_score * 0.25
        return min(max(overall, 0.0), 1.0)
    
    def _determine_engagement_level(
        self,
        facial: FacialAnalysis,
        voice: VoiceAnalysis,
        content: ContentAnalysis
    ) -> EngagementLevel:
        """ç¡®å®šå‚ä¸åº¦çº§åˆ«"""
        engagement_score = (
            facial.attention_level * 0.4 +
            facial.eye_contact * 0.3 +
            voice.tone_confidence * 0.2 +
            content.relevance_score * 0.1
        )
        
        if engagement_score >= 0.8:
            return EngagementLevel.HIGH
        elif engagement_score >= 0.6:
            return EngagementLevel.MEDIUM
        elif engagement_score >= 0.4:
            return EngagementLevel.LOW
        else:
            return EngagementLevel.DISTRACTED
    
    async def _generate_feedback(
        self,
        content: ContentAnalysis,
        facial: FacialAnalysis,
        voice: VoiceAnalysis,
        overall_score: float
    ) -> Tuple[List[str], List[str], List[str]]:
        """ç”Ÿæˆåé¦ˆå»ºè®®"""
        try:
            prompt = f"""
ä½œä¸ºé¢è¯•ä¸“å®¶ï¼ŒåŸºäºä»¥ä¸‹è¯„ä¼°æ•°æ®ç”Ÿæˆåé¦ˆï¼š

ç»¼åˆå¾—åˆ†: {overall_score:.2f}
å†…å®¹è´¨é‡: ç›¸å…³æ€§{content.relevance_score:.2f}, å®Œæ•´æ€§{content.completeness:.2f}
é¢éƒ¨è¡¨æƒ…: è‡ªä¿¡åº¦{facial.confidence:.2f}, çœ¼ç¥äº¤æµ{facial.eye_contact:.2f}
è¯­éŸ³è¡¨ç°: è¯­è°ƒè‡ªä¿¡{voice.tone_confidence:.2f}, æ¸…æ™°åº¦{voice.clarity:.2f}

è¯·ç”Ÿæˆï¼š
1. 3-5æ¡å…·ä½“çš„æ”¹è¿›å»ºè®®
2. 2-3ä¸ªè¡¨ç°ä¼˜åŠ¿
3. 2-3ä¸ªéœ€è¦æ”¹è¿›çš„åœ°æ–¹

ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "recommendations": ["å»ºè®®1", "å»ºè®®2"],
  "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
  "improvements": ["æ”¹è¿›ç‚¹1", "æ”¹è¿›ç‚¹2"]
}}
"""

            client = await self.client_manager.get_healthy_client(provider_type="doubao")
            if client:
                response = await client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=600,
                    temperature=0.2
                )
                
                result_json = json.loads(response.choices[0].message.content)
                return (
                    result_json.get("recommendations", []),
                    result_json.get("strengths", []),
                    result_json.get("improvements", [])
                )
        except Exception as e:
            logger.error(f"âŒ åé¦ˆç”Ÿæˆå¤±è´¥: {e}")
        
        # é»˜è®¤åé¦ˆ
        return (
            ["ä¿æŒè‡ªä¿¡çš„è¡¨è¾¾", "æ³¨æ„çœ¼ç¥äº¤æµ", "å›ç­”è¦æ›´åŠ å…·ä½“"],
            ["å›ç­”ç›¸å…³æ€§å¥½", "è¡¨è¾¾è¾ƒä¸ºæ¸…æ™°"],
            ["å¯ä»¥æ›´åŠ è‡ªä¿¡", "æ³¨æ„è¯­é€Ÿæ§åˆ¶"]
        )
    
    def _create_default_assessment(self) -> MultimodalAssessment:
        """åˆ›å»ºé»˜è®¤è¯„ä¼°ç»“æœ"""
        return MultimodalAssessment(
            overall_score=0.5,
            facial_analysis=FacialAnalysis(
                emotion=EmotionState.CALM, confidence=0.5, eye_contact=0.5,
                smile_intensity=0.5, attention_level=0.5, timestamp=time.time()
            ),
            voice_analysis=VoiceAnalysis(
                tone_confidence=0.5, speech_pace=150, volume_level=0.5,
                clarity=0.5, emotional_tone="å¹³é™", pause_frequency=0.3, timestamp=time.time()
            ),
            content_analysis=ContentAnalysis(
                relevance_score=0.5, completeness=0.5, technical_accuracy=0.5,
                communication_clarity=0.5, keywords_coverage=[], timestamp=time.time()
            ),
            engagement_level=EngagementLevel.MEDIUM,
            recommendations=["ä¿æŒè‡ªç„¶çš„è¡¨è¾¾"],
            strengths=["åŸºæœ¬è¡¨ç°æ­£å¸¸"],
            areas_for_improvement=["å¯ä»¥æ›´åŠ è‡ªä¿¡"],
            timestamp=time.time()
        )
    
    def get_session_summary(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯æ€»ç»“"""
        if not self.assessment_history:
            return {"message": "æš‚æ— è¯„ä¼°æ•°æ®"}
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_score = sum(a.overall_score for a in self.assessment_history) / len(self.assessment_history)
        
        # ç»Ÿè®¡æƒ…ç»ªåˆ†å¸ƒ
        emotions = [a.facial_analysis.emotion.value for a in self.assessment_history]
        emotion_counts = {emotion: emotions.count(emotion) for emotion in set(emotions)}
        
        # ç»Ÿè®¡å‚ä¸åº¦
        engagements = [a.engagement_level.value for a in self.assessment_history]
        engagement_counts = {level: engagements.count(level) for level in set(engagements)}
        
        return {
            "session_duration": time.time() - self.current_session["start_time"],
            "total_questions": len(self.assessment_history),
            "average_score": avg_score,
            "emotion_distribution": emotion_counts,
            "engagement_distribution": engagement_counts,
            "score_trend": [a.overall_score for a in self.assessment_history],
            "recommendations_summary": self._summarize_recommendations()
        }
    
    def _summarize_recommendations(self) -> List[str]:
        """æ€»ç»“æ¨èå»ºè®®"""
        all_recommendations = []
        for assessment in self.assessment_history:
            all_recommendations.extend(assessment.recommendations)
        
        # ç»Ÿè®¡é¢‘ç‡æœ€é«˜çš„å»ºè®®
        recommendation_counts = {}
        for rec in all_recommendations:
            recommendation_counts[rec] = recommendation_counts.get(rec, 0) + 1
        
        # è¿”å›å‰5ä¸ªæœ€å¸¸è§çš„å»ºè®®
        return sorted(recommendation_counts.keys(), 
                     key=lambda x: recommendation_counts[x], reverse=True)[:5]

# å…¨å±€å®ä¾‹
_multimodal_evaluator: Optional[MultimodalInterviewEvaluator] = None

def get_multimodal_evaluator() -> MultimodalInterviewEvaluator:
    """è·å–å¤šæ¨¡æ€è¯„ä¼°å™¨å®ä¾‹"""
    global _multimodal_evaluator
    
    if _multimodal_evaluator is None:
        _multimodal_evaluator = MultimodalInterviewEvaluator()
    
    return _multimodal_evaluator

# ä¾¿æ·å‡½æ•°
async def evaluate_interview_response(
    question: str,
    answer: str,
    image_data: Optional[bytes] = None,
    audio_data: Optional[bytes] = None
) -> MultimodalAssessment:
    """ä¾¿æ·çš„é¢è¯•å›ç­”è¯„ä¼°å‡½æ•°"""
    evaluator = get_multimodal_evaluator()
    return await evaluator.comprehensive_assessment(question, answer, image_data, audio_data)

async def start_interview(position_type: str = "general", expected_skills: List[str] = None):
    """å¼€å§‹é¢è¯•ä¼šè¯"""
    evaluator = get_multimodal_evaluator()
    evaluator.start_interview_session(position_type, expected_skills)

def get_interview_summary() -> Dict[str, Any]:
    """è·å–é¢è¯•æ€»ç»“"""
    evaluator = get_multimodal_evaluator()
    return evaluator.get_session_summary() 