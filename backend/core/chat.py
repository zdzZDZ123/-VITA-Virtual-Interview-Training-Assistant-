"""
LLM èŠå¤©å°è£…æ¨¡å—
"""
import logging
import time
from typing import List, Dict, Optional, Any, AsyncGenerator, Tuple
from core.prompts import SYSTEM_PROMPT, FEEDBACK_PROMPT, get_interview_prompt
from .config import config
from core.qwen_llama_client import create_http_client, safe_chat_completion, initialize_clients, get_client_manager
from .performance_monitor import get_performance_monitor, PerformanceContext, async_timeit
from core.error_handler import with_retry, RetryConfig, handle_errors, ErrorCategory
from .health_monitor import record_request_metrics
from core.cache_manager import get_cache, cache_key_for_chat
import asyncio
import json
import uuid
from datetime import datetime, timedelta
import threading
import weakref

logger = logging.getLogger(__name__)

# é¢„è®¾é¢è¯•é¢˜åº“ - ä½œä¸ºAPIä¸å¯ç”¨æ—¶çš„å›é€€
FALLBACK_QUESTIONS = {
    "technical": [
        "è¯·è§£é‡Šä¸€ä¸‹ JavaScript ä¸­çš„äº‹ä»¶å¾ªç¯ (event loop) ã€‚",
        "ä»€ä¹ˆæ˜¯ React çš„è™šæ‹ŸDOMï¼Ÿå®ƒå¦‚ä½•æé«˜æ€§èƒ½ï¼Ÿ",
        "è¯·æè¿° HTTP å’Œ HTTPS çš„åŒºåˆ«ï¼Œä»¥åŠ HTTPS çš„å·¥ä½œåŸç†ã€‚",
        "ä»€ä¹ˆæ˜¯å“åº”å¼è®¾è®¡ï¼Ÿè¯·è¯´æ˜å…¶æ ¸å¿ƒåŸåˆ™ã€‚",
        "è¯·è§£é‡Š TypeScript ç›¸æ¯” JavaScript çš„ä¼˜åŠ¿ã€‚",
        "ä»€ä¹ˆæ˜¯ RESTful APIï¼Ÿè¯·è¯´æ˜å…¶è®¾è®¡åŸåˆ™ã€‚",
        "è¯·æè¿°å‰ç«¯æ€§èƒ½ä¼˜åŒ–çš„å¸¸è§æ–¹æ³•ã€‚",
        "ä»€ä¹ˆæ˜¯ Git çš„åˆ†æ”¯ç­–ç•¥ï¼Ÿè¯·è¯´æ˜å¸¸è§çš„å·¥ä½œæµç¨‹ã€‚",
        "è¯·è§£é‡Š CSS ç›’æ¨¡å‹çš„æ¦‚å¿µã€‚",
        "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿè¯·æ¯”è¾ƒ Promise å’Œ async/awaitã€‚"
    ],
    "behavioral": [
        "è¯·æè¿°ä¸€ä¸ªä½ é‡åˆ°çš„æœ€å…·æŒ‘æˆ˜æ€§çš„é¡¹ç›®ï¼Œä»¥åŠä½ æ˜¯å¦‚ä½•è§£å†³é—®é¢˜çš„ã€‚",
        "è°ˆè°ˆä½ åœ¨å›¢é˜Ÿåˆä½œä¸­é‡åˆ°çš„å›°éš¾ï¼Œä»¥åŠä½ æ˜¯å¦‚ä½•å¤„ç†çš„ã€‚",
        "æè¿°ä¸€æ¬¡ä½ å¿…é¡»åœ¨ç´§è¿«çš„æˆªæ­¢æ—¥æœŸå‰å®Œæˆå·¥ä½œçš„ç»å†ã€‚",
        "è¯·ä¸¾ä¾‹è¯´æ˜ä½ å¦‚ä½•å¤„ç†ä¸åŒäº‹æ„è§ä¸åˆçš„æƒ…å†µã€‚",
        "è°ˆè°ˆä½ æœ€å¼•ä»¥ä¸ºè±ªçš„ä¸€ä¸ªå·¥ä½œæˆå°±ã€‚",
        "æè¿°ä¸€æ¬¡ä½ ä»å¤±è´¥ä¸­å­¦åˆ°é‡è¦ç»éªŒçš„ç»å†ã€‚",
        "è¯·è¯´æ˜ä½ å¦‚ä½•å¹³è¡¡å·¥ä½œå’Œä¸ªäººç”Ÿæ´»ã€‚",
        "è°ˆè°ˆä½ å¦‚ä½•æŒç»­å­¦ä¹ å’Œæå‡è‡ªå·±çš„æŠ€èƒ½ã€‚",
        "æè¿°ä¸€æ¬¡ä½ éœ€è¦å¿«é€Ÿé€‚åº”æ–°æŠ€æœ¯æˆ–æ–°ç¯å¢ƒçš„ç»å†ã€‚",
        "è¯·ä¸¾ä¾‹è¯´æ˜ä½ å¦‚ä½•ç»™å›¢é˜Ÿæˆå‘˜æä¾›å¸®åŠ©æˆ–æŒ‡å¯¼ã€‚"
    ],
    "situational": [
        "å¦‚æœä½ å‘ç°ä»£ç ä¸­æœ‰ä¸€ä¸ªä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œä½†ä¿®å¤å®ƒå¯èƒ½å½±å“é¡¹ç›®è¿›åº¦ï¼Œä½ ä¼šæ€ä¹ˆå¤„ç†ï¼Ÿ",
        "å‡è®¾ä½ çš„ç›´æ¥ä¸Šçº§ç»™ä½ åˆ†é…äº†ä¸€ä¸ªä½ è®¤ä¸ºä¸åˆç†çš„ä»»åŠ¡ï¼Œä½ ä¼šå¦‚ä½•åº”å¯¹ï¼Ÿ",
        "å¦‚æœå®¢æˆ·è¦æ±‚åœ¨å¾ˆçŸ­æ—¶é—´å†…äº¤ä»˜ä¸€ä¸ªå¤æ‚åŠŸèƒ½ï¼Œä½ ä¼šå¦‚ä½•è§„åˆ’å’Œæ‰§è¡Œï¼Ÿ",
        "å‡è®¾ä½ å‘ç°å›¢é˜Ÿæˆå‘˜çš„ä»£ç è´¨é‡ä¸ç¬¦åˆæ ‡å‡†ï¼Œä½ ä¼šå¦‚ä½•å¤„ç†ï¼Ÿ",
        "å¦‚æœåœ¨é¡¹ç›®è¿›è¡Œä¸­å‘ç°éœ€æ±‚æœ‰é‡å¤§å˜åŒ–ï¼Œä½ ä¼šé‡‡å–ä»€ä¹ˆæªæ–½ï¼Ÿ",
        "å‡è®¾ä½ éœ€è¦åœ¨å¤šä¸ªç´§æ€¥ä»»åŠ¡ä¹‹é—´åšé€‰æ‹©ï¼Œä½ ä¼šå¦‚ä½•ç¡®å®šä¼˜å…ˆçº§ï¼Ÿ",
        "å¦‚æœä½ å¯¹æŸä¸ªæŠ€æœ¯å†³ç­–æœ‰ä¸åŒæ„è§ï¼Œä½†å›¢é˜Ÿå·²ç»å†³å®šï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ",
        "å‡è®¾é¡¹ç›®å‡ºç°ä¸¥é‡bugå½±å“ç”¨æˆ·ä½“éªŒï¼Œä½ ä¼šå¦‚ä½•å¿«é€Ÿå“åº”ï¼Ÿ",
        "å¦‚æœä½ éœ€è¦å‘éæŠ€æœ¯äººå‘˜è§£é‡Šå¤æ‚çš„æŠ€æœ¯é—®é¢˜ï¼Œä½ ä¼šé‡‡ç”¨ä»€ä¹ˆæ–¹æ³•ï¼Ÿ",
        "å‡è®¾ä½ å‘ç°ç°æœ‰ç³»ç»Ÿæ¶æ„å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œä½ ä¼šå¦‚ä½•æå‡ºæ”¹è¿›å»ºè®®ï¼Ÿ"
    ]
}

class InterviewSession:
    """é¢è¯•ä¼šè¯ç®¡ç†"""
    def __init__(self, session_id: str, job_description: str, interview_type: str):
        self.session_id = session_id
        self.job_description = job_description
        self.interview_type = interview_type
        self.questions_asked = []
        self.answers = []
        self.current_question_index = 0
        self.created_at = datetime.now()
        self.last_activity = datetime.now()  # æ·»åŠ æœ€åæ´»åŠ¨æ—¶é—´
        self.using_fallback = False
        
    def add_qa_pair(self, question: str, answer: Optional[str] = None):
        """æ·»åŠ é—®ç­”å¯¹"""
        self.last_activity = datetime.now()  # æ›´æ–°æ´»åŠ¨æ—¶é—´
        if answer is None:
            # åªæ·»åŠ é—®é¢˜
            self.questions_asked.append(question)
        else:
            # æ·»åŠ ç­”æ¡ˆåˆ°å¯¹åº”é—®é¢˜
            if len(self.answers) < len(self.questions_asked):
                self.answers.append(answer)

class ChatService:
    """å°è£… LLM è°ƒç”¨é€»è¾‘ï¼Œæ”¯æŒæ™ºèƒ½æ¨¡å‹é€‰æ‹©ã€Qwenä¼˜å…ˆæ¶æ„å’Œä¼˜åŒ–é…ç½®"""
    
    def __init__(self):
        try:
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            initialize_clients()
            
            # ä½¿ç”¨å®¢æˆ·ç«¯ç®¡ç†å™¨è·å–å®¢æˆ·ç«¯
            self.client_manager = get_client_manager()
            self.performance_monitor = get_performance_monitor()
            
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„ä¼šè¯ç®¡ç†
            self._sessions_lock = threading.RLock()
            self.sessions: Dict[str, InterviewSession] = {}
            self.fallback_questions = FALLBACK_QUESTIONS
            
            # ä¼šè¯æ¸…ç†é…ç½®
            self.max_sessions = 1000  # æœ€å¤§ä¼šè¯æ•°
            self.session_timeout_hours = 24  # ä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            self.cleanup_interval = 3600  # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
            self._last_cleanup = time.time()
            
            # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
            self._cleanup_task = None
            self._start_cleanup_task()
            
            logger.info("ChatService åˆå§‹åŒ–æˆåŠŸ (Qwenä¼˜å…ˆæ¶æ„)")
        except Exception as e:
            logger.error(f"ChatService åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _start_cleanup_task(self):
        """å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡"""
        try:
            async def cleanup_loop():
                while True:
                    try:
                        await asyncio.sleep(self.cleanup_interval)
                        await self._cleanup_expired_sessions()
                    except Exception as e:
                        logger.warning(f"ä¼šè¯æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_running_loop()
                self._cleanup_task = asyncio.create_task(cleanup_loop())
                logger.info("âœ… ä¼šè¯æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç¨åå¯åŠ¨
                logger.debug("â³ ç­‰å¾…äº‹ä»¶å¾ªç¯å¯åŠ¨åå†å¯åŠ¨æ¸…ç†ä»»åŠ¡")
        except Exception as e:
            logger.warning(f"å¯åŠ¨æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
    
    async def _cleanup_expired_sessions(self):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        try:
            current_time = datetime.now()
            timeout_delta = timedelta(hours=self.session_timeout_hours)
            expired_sessions = []
            
            with self._sessions_lock:
                # æ‰¾åˆ°è¿‡æœŸä¼šè¯
                for session_id, session in list(self.sessions.items()):
                    if current_time - session.last_activity > timeout_delta:
                        expired_sessions.append(session_id)
                
                # åˆ é™¤è¿‡æœŸä¼šè¯
                for session_id in expired_sessions:
                    del self.sessions[session_id]
                
                # å¦‚æœä¼šè¯æ•°é‡ä»ç„¶è¿‡å¤šï¼Œåˆ é™¤æœ€æ—§çš„ä¼šè¯
                if len(self.sessions) > self.max_sessions:
                    # æŒ‰æœ€åæ´»åŠ¨æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
                    sessions_by_activity = sorted(
                        self.sessions.items(),
                        key=lambda x: x[1].last_activity
                    )
                    
                    excess_count = len(self.sessions) - self.max_sessions
                    for i in range(excess_count):
                        session_id = sessions_by_activity[i][0]
                        del self.sessions[session_id]
                        expired_sessions.append(session_id)
            
            if expired_sessions:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_sessions)} ä¸ªè¿‡æœŸ/è¿‡å¤šä¼šè¯")
            
            self._last_cleanup = time.time()
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸä¼šè¯å¤±è´¥: {e}")
    
    def _ensure_cleanup_task(self):
        """ç¡®ä¿æ¸…ç†ä»»åŠ¡æ­£åœ¨è¿è¡Œ"""
        if self._cleanup_task is None or self._cleanup_task.done():
            self._start_cleanup_task()
    
    @async_timeit(metric_name="llm.ask", log_slow_threshold=2.0)
    async def ask_llm(
        self, 
        messages: List[Dict[str, str]], 
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 256,
        task_type: str = "chat"
    ) -> str:
        """
        è°ƒç”¨ LLM è·å–å›å¤ï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œæ”¯æŒQwenä¼˜å…ˆæ¶æ„è‡ªåŠ¨åˆ‡æ¢
        
        Args:
            messages: å¯¹è¯å†å²ï¼Œæ ¼å¼ [{"role": "user|assistant|system", "content": "..."}]
            model: æŒ‡å®šæ¨¡å‹åç§°ï¼Œå¦‚æœæœªæä¾›åˆ™æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©
            temperature: éšæœºæ€§æ§åˆ¶
            max_tokens: æœ€å¤§tokenæ•°
            task_type: ä»»åŠ¡ç±»å‹ï¼Œç”¨äºè‡ªåŠ¨æ¨¡å‹é€‰æ‹©
            
        Returns:
            LLM ç”Ÿæˆçš„å›å¤å†…å®¹
        """
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨Qwenå®¢æˆ·ç«¯ï¼ˆç°åœ¨æ˜¯ä¸»è¦æä¾›å•†ï¼‰
            client = await self.client_manager.get_healthy_client(provider_type='qwen')
            
            if client:
                # ä½¿ç”¨Qwenæ¨¡å‹
                selected_model = model or config.get_model_for_provider('qwen', task_type)
                logger.info(f"ä½¿ç”¨Qwençš„{selected_model}æ¨¡å‹å¤„ç†{task_type}ä»»åŠ¡")
                
                try:
                    async with PerformanceContext(
                        self.performance_monitor,
                        provider='qwen',
                        function_type=task_type
                    ):
                        response = await safe_chat_completion(
                            client,
                            model=selected_model,
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        )
                        return response.choices[0].message.content.strip()
                except Exception as e:
                    logger.error(f"Qwenè°ƒç”¨å¤±è´¥: {str(e)}")
                    # å¦‚æœQwenè°ƒç”¨å¤±è´¥ï¼Œç»§ç»­å°è¯•Llama
                    if not (config.USE_LLAMA_FALLBACK and config.ENABLE_AUTO_SWITCH):
                        raise
            
            # å¦‚æœQwenä¸å¯ç”¨æˆ–è°ƒç”¨å¤±è´¥ï¼Œä¸”å¯ç”¨äº†å¤‡ä»½ï¼Œå°è¯•Llama
            if config.USE_LLAMA_FALLBACK and config.ENABLE_AUTO_SWITCH:
                logger.info("å°è¯•ä½¿ç”¨Llamaå¤‡ä»½å®¢æˆ·ç«¯")
                fallback_client = await self.client_manager.get_healthy_client(provider_type='llama')
                
                if fallback_client:
                    # ä½¿ç”¨Llamaæ¨¡å‹
                    selected_model = model or config.get_model_for_provider('llama', task_type)
                    logger.info(f"ä½¿ç”¨Llamaçš„{selected_model}æ¨¡å‹å¤„ç†{task_type}ä»»åŠ¡")
                    
                    async with PerformanceContext(
                        self.performance_monitor,
                        provider='llama',
                        function_type=task_type
                    ):
                        response = await safe_chat_completion(
                            fallback_client,
                            model=selected_model,
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        )
                        return response.choices[0].message.content.strip()
            
            # å¦‚æœéƒ½å¤±è´¥äº†
            raise Exception("æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    
    async def generate_interview_question(
        self, 
        job_description: str, 
        interview_type: str = "technical",
        previous_qa: List[Tuple[str, str]] = None,
        question_number: int = 1,
        conversation_history: List[Dict[str, str]] = None  # å‘åå…¼å®¹çš„å‚æ•°
    ) -> str:
        """
        ç”Ÿæˆé¢è¯•é—®é¢˜
        ä¼˜å…ˆä½¿ç”¨AIæ¨¡å‹ï¼Œå¤±è´¥æ—¶ä½¿ç”¨é¢„è®¾é—®é¢˜
        """
        try:
            # å°è¯•ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆé—®é¢˜
            ai_question = await self._generate_ai_question(
                job_description, interview_type, previous_qa, question_number
            )
            if ai_question:
                return ai_question
        except Exception as e:
            logger.warning(f"AIé—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å›é€€é—®é¢˜: {e}")
        
        # ä½¿ç”¨é¢„è®¾é—®é¢˜ä½œä¸ºå›é€€
        return self._get_fallback_question(interview_type, question_number)
    
    async def _generate_ai_question(
        self,
        job_description: str,
        interview_type: str,
        previous_qa: List[Tuple[str, str]] = None,
        question_number: int = 1
    ) -> Optional[str]:
        """ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆé—®é¢˜"""
        try:
            # ä¼˜å…ˆå°è¯•Qwenï¼Œç„¶åå°è¯•Llama
            for provider in ['qwen', 'llama']:
                try:
                    client = await self.client_manager.get_healthy_client(provider_type=provider)
                    if not client:
                        continue
                    
                    model = config.get_model_for_provider(provider, "interview")
                    prompt = get_interview_prompt(
                        job_description, interview_type, previous_qa, question_number
                    )
                    
                    response = await safe_chat_completion(
                        client,
                        model=model,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=500,
                        temperature=0.7
                    )
                    
                    if response and response.choices:
                        question = response.choices[0].message.content.strip()
                        logger.info(f"âœ… ä½¿ç”¨{provider}æ¨¡å‹ç”Ÿæˆé—®é¢˜æˆåŠŸ")
                        return question
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ {provider}æ¨¡å‹ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                    continue
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ AIé—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _get_fallback_question(self, interview_type: str, question_number: int) -> str:
        """è·å–é¢„è®¾å›é€€é—®é¢˜ï¼Œæ”¯æŒæ™ºèƒ½é€‰æ‹©"""
        questions = self.fallback_questions.get(interview_type, self.fallback_questions["technical"])
        
        # å¾ªç¯ä½¿ç”¨é—®é¢˜ï¼Œé¿å…è¶…å‡ºèŒƒå›´
        index = (question_number - 1) % len(questions)
        question = questions[index]
        
        # ä¸ºé—®é¢˜æ·»åŠ å˜åŒ–ä»¥é¿å…é‡å¤æ„Ÿ
        if question_number > len(questions):
            # å½“é—®é¢˜æ•°è¶…è¿‡é¢„è®¾æ•°é‡æ—¶ï¼Œæ·»åŠ åºå·å˜åŒ–
            cycle_num = (question_number - 1) // len(questions) + 1
            if cycle_num > 1:
                question = f"[æ·±å…¥] {question}"
        
        logger.info(f"ğŸ“ ä½¿ç”¨æœ¬åœ°é—®é¢˜åº“ ({interview_type}, #{question_number}): {question[:50]}...")
        return question
    
    async def start_interview_session(
        self, 
        job_description: str, 
        interview_type: str = "technical"
    ) -> Dict[str, Any]:
        """å¼€å§‹é¢è¯•ä¼šè¯"""
        # ç¡®ä¿æ¸…ç†ä»»åŠ¡åœ¨è¿è¡Œ
        self._ensure_cleanup_task()
        
        session_id = str(uuid.uuid4())
        
        # åˆ›å»ºä¼šè¯
        session = InterviewSession(session_id, job_description, interview_type)
        
        with self._sessions_lock:
            self.sessions[session_id] = session
        
        # ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜
        first_question = await self.generate_interview_question(
            job_description, interview_type, question_number=1
        )
        
        session.add_qa_pair(first_question)
        
        logger.info(f"ğŸš€ é¢è¯•ä¼šè¯å·²åˆ›å»º: {session_id}")
        
        return {
            "session_id": session_id,
            "first_question": first_question,
            "interview_type": interview_type,
            "created_at": session.created_at.isoformat()
        }
    
    async def __REMOVED_API_KEY__(
        self, 
        session_id: str, 
        answer: str
    ) -> Dict[str, Any]:
        """å¤„ç†ç­”æ¡ˆå¹¶ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
        with self._sessions_lock:
            if session_id not in self.sessions:
                raise ValueError("ä¼šè¯ä¸å­˜åœ¨")
            
            session = self.sessions[session_id]
        
        # è®°å½•ç­”æ¡ˆ
        session.add_qa_pair(None, answer)
        session.current_question_index += 1
        
        # ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
        previous_qa = list(zip(session.questions_asked, session.answers))
        next_question = await self.generate_interview_question(
            session.job_description,
            session.interview_type,
            previous_qa,
            session.current_question_index + 1
        )
        
        session.add_qa_pair(next_question)
        
        return {
            "question": next_question,
            "question_number": session.current_question_index + 1,
            "total_questions": len(session.questions_asked)
        }
    
    def get_session(self, session_id: str) -> Optional[InterviewSession]:
        """è·å–ä¼šè¯ä¿¡æ¯"""
        with self._sessions_lock:
            return self.sessions.get(session_id)
    
    def end_session(self, session_id: str) -> Dict[str, Any]:
        """ç»“æŸä¼šè¯å¹¶ç”Ÿæˆæ€»ç»“"""
        with self._sessions_lock:
            if session_id not in self.sessions:
                raise ValueError("ä¼šè¯ä¸å­˜åœ¨")
            
            session = self.sessions[session_id]
            
            # ç®€å•çš„æ€»ç»“
            summary = {
                "session_id": session_id,
                "interview_type": session.interview_type,
                "questions_count": len(session.questions_asked),
                "answers_count": len(session.answers),
                "duration": (datetime.now() - session.created_at).total_seconds(),
                "completed": len(session.answers) >= len(session.questions_asked)
            }
            
            # æ¸…ç†ä¼šè¯
            del self.sessions[session_id]
        
        return summary
    
    def get_session_stats(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        with self._sessions_lock:
            return {
                "total_sessions": len(self.sessions),
                "max_sessions": self.max_sessions,
                "cleanup_interval_seconds": self.cleanup_interval,
                "session_timeout_hours": self.session_timeout_hours,
                "last_cleanup": self._last_cleanup
            }

    async def generate_feedback_report(
        self, 
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        ç”Ÿæˆé¢è¯•åé¦ˆæŠ¥å‘Šï¼Œä½¿ç”¨æœ€å¼ºæ¨¡å‹ç¡®ä¿åˆ†æè´¨é‡
        
        Args:
            conversation_history: å®Œæ•´çš„å¯¹è¯å†å²
            
        Returns:
            ç»“æ„åŒ–çš„åé¦ˆæŠ¥å‘Š
        """
        # å°†å¯¹è¯å†å²è½¬æ¢ä¸ºæ–‡æœ¬
        transcript = "\n".join([
            f"{'é¢è¯•å®˜' if h['role'] == 'assistant' else 'å€™é€‰äºº'}: {h['content']}" 
            for h in conversation_history if h['role'] in ['user', 'assistant']
        ])
        
        messages = [
            {"role": "system", "content": FEEDBACK_PROMPT},
            {"role": "user", "content": f"é¢è¯•è®°å½•ï¼š\n{transcript}"}
        ]
        
        # ä½¿ç”¨analysisä»»åŠ¡ç±»å‹ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€å¼ºæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
        return await self.ask_llm(
            messages, 
            temperature=0.5, 
            max_tokens=1024,
            task_type="analysis"
        )
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # å–æ¶ˆæ¸…ç†ä»»åŠ¡
            if self._cleanup_task and not self._cleanup_task.done():
                self._cleanup_task.cancel()
                try:
                    await self._cleanup_task
                except asyncio.CancelledError:
                    pass
            
            # æ¸…ç†æ‰€æœ‰ä¼šè¯
            with self._sessions_lock:
                self.sessions.clear()
            
            logger.info("âœ… ChatServiceèµ„æºå·²æ¸…ç†")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ChatServiceæ¸…ç†æ—¶å‡ºç°è­¦å‘Š: {e}")

@with_retry(RetryConfig(max_retries=3, base_delay=1.0))
@handle_errors(category=ErrorCategory.API)
async def chat_completion(
    messages: List[Dict[str, str]],
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    stream: bool = False
) -> Dict[str, Any]:
    """ç»Ÿä¸€çš„èŠå¤©å®Œæˆæ¥å£"""
    start_time = time.time()
    success = False
    
    try:
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = cache_key_for_chat(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # å°è¯•ä»ç¼“å­˜è·å–å“åº”ï¼ˆä»…å¯¹éæµå¼è¯·æ±‚ï¼‰
        if not stream:
            cache = get_cache("chat")
            cached_response = cache.get(cache_key)
            if cached_response:
                success = True
                return cached_response
        
        # è·å–å¥åº·çš„å®¢æˆ·ç«¯
        client = await get_healthy_client()
        if not client:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„å¥åº·å®¢æˆ·ç«¯")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        if not model:
            model = client.get_default_model()
        
        # è°ƒç”¨èŠå¤©å®Œæˆ
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream
        )
        
        success = True
        
        if stream:
            return response
        else:
            result = {
                "content": response.choices[0].message.content,
                "model": response.model,
                "usage": response.usage.dict() if response.usage else None
            }
            
            # ç¼“å­˜å“åº”
            cache = get_cache("chat")
            cache.set(cache_key, result)
            
            return result
            
    except Exception as e:
        logger.error(f"èŠå¤©å®Œæˆå¤±è´¥: {e}")
        raise e
    finally:
        # è®°å½•è¯·æ±‚æŒ‡æ ‡
        response_time = time.time() - start_time
        record_request_metrics(response_time, success)

# å…¨å±€å•ä¾‹
chat_service = ChatService()

def get_chat_service() -> ChatService:
    """è·å–èŠå¤©æœåŠ¡å®ä¾‹"""
    global chat_service
    return chat_service