"""
å¤šæ¨¡æ€é¢è¯•API
æä¾›åŸºäºDoubao-Seed-1.6çš„é¢è¯•è¯„ä¼°æ¥å£
"""

from fastapi import APIRouter, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Optional, List
import logging
import json
import base64
from io import BytesIO
from PIL import Image
import time

from core.multimodal_interview import (
    get_multimodal_evaluator, 
    evaluate_interview_response,
    start_interview,
    get_interview_summary,
    MultimodalAssessment
)
from core.config import VITAConfig

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/multimodal", tags=["å¤šæ¨¡æ€é¢è¯•"])

@router.post("/start-interview")
async def start_interview_session(
    position_type: str = Form(default="general"),
    expected_skills: Optional[str] = Form(default=None)
):
    """
    å¼€å§‹å¤šæ¨¡æ€é¢è¯•ä¼šè¯
    
    Args:
        position_type: èŒä½ç±»å‹ (general, software_engineer, data_scientist, etc.)
        expected_skills: æœŸæœ›æŠ€èƒ½åˆ—è¡¨ï¼ŒJSONå­—ç¬¦ä¸²æ ¼å¼
    """
    try:
        skills_list = []
        if expected_skills:
            try:
                skills_list = json.loads(expected_skills)
            except json.JSONDecodeError:
                skills_list = [skill.strip() for skill in expected_skills.split(",")]
        
        await start_interview(position_type, skills_list)
        
        return JSONResponse({
            "success": True,
            "message": f"é¢è¯•ä¼šè¯å·²å¼€å§‹",
            "session_info": {
                "position_type": position_type,
                "expected_skills": skills_list,
                "evaluator_model": "Doubao-Seed-1.6"
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ å¼€å§‹é¢è¯•ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¼€å§‹é¢è¯•ä¼šè¯å¤±è´¥: {str(e)}")

@router.post("/evaluate-response")
async def evaluate_multimodal_response(
    question: str = Form(..., description="é¢è¯•é—®é¢˜"),
    answer: str = Form(..., description="ç”¨æˆ·å›ç­”"),
    image: Optional[UploadFile] = File(None, description="ç”¨æˆ·é¢éƒ¨å›¾åƒ"),
    audio: Optional[UploadFile] = File(None, description="ç”¨æˆ·è¯­éŸ³æ–‡ä»¶"),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """
    å¤šæ¨¡æ€é¢è¯•å›ç­”è¯„ä¼°
    
    ä½¿ç”¨Doubao-Seed-1.6æ¨¡å‹åˆ†æç”¨æˆ·çš„ï¼š
    - é¢éƒ¨è¡¨æƒ…å’Œè‚¢ä½“è¯­è¨€ï¼ˆå¦‚æœæä¾›å›¾åƒï¼‰
    - è¯­éŸ³è¯­è°ƒå’Œæƒ…æ„Ÿï¼ˆå¦‚æœæä¾›éŸ³é¢‘ï¼‰
    - å›ç­”å†…å®¹è´¨é‡
    """
    try:
        # å¤„ç†å›¾åƒæ•°æ®
        image_data = None
        if image:
            if image.content_type not in ["image/jpeg", "image/png", "image/jpg"]:
                raise HTTPException(status_code=400, detail="ä»…æ”¯æŒJPEG/PNGå›¾åƒæ ¼å¼")
            
            # è¯»å–å¹¶å¤„ç†å›¾åƒ
            image_bytes = await image.read()
            
            # éªŒè¯å›¾åƒå¹¶è°ƒæ•´å¤§å°ï¼ˆé¿å…è¿‡å¤§çš„å›¾åƒï¼‰
            try:
                pil_image = Image.open(BytesIO(image_bytes))
                # å¦‚æœå›¾åƒè¿‡å¤§ï¼Œè°ƒæ•´å¤§å°
                if pil_image.width > 1024 or pil_image.height > 1024:
                    pil_image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)
                    
                # è½¬æ¢ä¸ºJPEGæ ¼å¼ä»¥å‡å°‘å¤§å°
                output_buffer = BytesIO()
                pil_image.convert('RGB').save(output_buffer, format='JPEG', quality=85)
                image_data = output_buffer.getvalue()
            except Exception as e:
                logger.error(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
                raise HTTPException(status_code=400, detail=f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
        
        # å¤„ç†éŸ³é¢‘æ•°æ®
        audio_data = None
        if audio:
            if not audio.content_type.startswith("audio/"):
                raise HTTPException(status_code=400, detail="ä»…æ”¯æŒéŸ³é¢‘æ–‡ä»¶")
            
            audio_data = await audio.read()
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶25MBï¼‰
            if len(audio_data) > 25 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼Œé™åˆ¶25MB")
        
        # æ‰§è¡Œå¤šæ¨¡æ€è¯„ä¼°
        logger.info(f"ğŸ­ å¼€å§‹å¤šæ¨¡æ€è¯„ä¼°: é—®é¢˜é•¿åº¦={len(question)}, å›ç­”é•¿åº¦={len(answer)}, "
                   f"æœ‰å›¾åƒ={'æ˜¯' if image_data else 'å¦'}, æœ‰éŸ³é¢‘={'æ˜¯' if audio_data else 'å¦'}")
        
        assessment = await evaluate_interview_response(
            question=question,
            answer=answer,
            image_data=image_data,
            audio_data=audio_data
        )
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "success": True,
            "assessment": {
                "overall_score": round(assessment.overall_score, 3),
                "score_breakdown": {
                    "content_quality": {
                        "relevance": round(assessment.content_analysis.relevance_score, 3),
                        "completeness": round(assessment.content_analysis.completeness, 3),
                        "technical_accuracy": round(assessment.content_analysis.technical_accuracy, 3),
                        "communication_clarity": round(assessment.content_analysis.communication_clarity, 3),
                        "keywords_covered": assessment.content_analysis.keywords_coverage
                    },
                    "facial_expression": {
                        "emotion": assessment.facial_analysis.emotion.value,
                        "confidence": round(assessment.facial_analysis.confidence, 3),
                        "eye_contact": round(assessment.facial_analysis.eye_contact, 3),
                        "attention_level": round(assessment.facial_analysis.attention_level, 3)
                    } if image_data else None,
                    "voice_characteristics": {
                        "tone_confidence": round(assessment.voice_analysis.tone_confidence, 3),
                        "speech_pace": assessment.voice_analysis.speech_pace,
                        "clarity": round(assessment.voice_analysis.clarity, 3),
                        "emotional_tone": assessment.voice_analysis.emotional_tone
                    } if audio_data else None
                },
                "engagement_level": assessment.engagement_level.value,
                "feedback": {
                    "strengths": assessment.strengths,
                    "improvements": assessment.areas_for_improvement,
                    "recommendations": assessment.recommendations
                },
                "analysis_timestamp": assessment.timestamp
            },
            "model_info": {
                "evaluator_model": "Doubao-Seed-1.6",
                "analysis_components": {
                    "facial_analysis": image_data is not None,
                    "voice_analysis": audio_data is not None,
                    "content_analysis": True
                }
            }
        }
        
        logger.info(f"âœ… å¤šæ¨¡æ€è¯„ä¼°å®Œæˆï¼Œç»¼åˆå¾—åˆ†: {assessment.overall_score:.3f}")
        return JSONResponse(response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¤šæ¨¡æ€è¯„ä¼°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯„ä¼°å¤±è´¥: {str(e)}")

@router.get("/session-summary")
async def get_session_summary():
    """
    è·å–å½“å‰é¢è¯•ä¼šè¯çš„æ€»ç»“æŠ¥å‘Š
    """
    try:
        summary = get_interview_summary()
        
        if "message" in summary:
            return JSONResponse({
                "success": False,
                "message": summary["message"]
            })
        
        return JSONResponse({
            "success": True,
            "summary": {
                "session_stats": {
                    "duration_seconds": round(summary.get("session_duration", 0), 1),
                    "total_questions": summary.get("total_questions", 0),
                    "average_score": round(summary.get("average_score", 0), 3)
                },
                "performance_analysis": {
                    "score_trend": summary.get("score_trend", []),
                    "emotion_distribution": summary.get("emotion_distribution", {}),
                    "engagement_distribution": summary.get("engagement_distribution", {})
                },
                "key_recommendations": summary.get("recommendations_summary", []),
                "model_info": {
                    "evaluator_model": "Doubao-Seed-1.6",
                    "capabilities": [
                        "é¢éƒ¨è¡¨æƒ…åˆ†æ",
                        "è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«", 
                        "å†…å®¹è´¨é‡è¯„ä¼°",
                        "ç»¼åˆè¡¨ç°è¯„åˆ†"
                    ]
                }
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯æ€»ç»“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ€»ç»“å¤±è´¥: {str(e)}")

@router.post("/analyze-expression")
async def analyze_facial_expression(
    image: UploadFile = File(..., description="ç”¨æˆ·é¢éƒ¨å›¾åƒ")
):
    """
    å•ç‹¬åˆ†æé¢éƒ¨è¡¨æƒ…
    
    ä½¿ç”¨Doubao-Seed-1.6æ¨¡å‹åˆ†æç”¨æˆ·çš„é¢éƒ¨è¡¨æƒ…ã€çœ¼ç¥äº¤æµç­‰
    """
    try:
        if image.content_type not in ["image/jpeg", "image/png", "image/jpg"]:
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒJPEG/PNGå›¾åƒæ ¼å¼")
        
        # å¤„ç†å›¾åƒ
        image_bytes = await image.read()
        
        evaluator = get_multimodal_evaluator()
        facial_analysis = await evaluator.analyze_facial_expression(image_bytes)
        
        return JSONResponse({
            "success": True,
            "facial_analysis": {
                "emotion": facial_analysis.emotion.value,
                "confidence": round(facial_analysis.confidence, 3),
                "eye_contact": round(facial_analysis.eye_contact, 3),
                "smile_intensity": round(facial_analysis.smile_intensity, 3),
                "attention_level": round(facial_analysis.attention_level, 3),
                "timestamp": facial_analysis.timestamp
            },
            "model_info": {
                "analyzer_model": "Doubao-Seed-1.6",
                "analysis_type": "facial_expression"
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ é¢éƒ¨è¡¨æƒ…åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@router.post("/analyze-voice")
async def analyze_voice_characteristics(
    audio: UploadFile = File(..., description="ç”¨æˆ·è¯­éŸ³æ–‡ä»¶"),
    transcript: str = Form(..., description="è¯­éŸ³è½¬å½•æ–‡æœ¬")
):
    """
    å•ç‹¬åˆ†æè¯­éŸ³ç‰¹å¾
    
    ä½¿ç”¨Doubao-Seed-1.6æ¨¡å‹åˆ†æç”¨æˆ·çš„è¯­éŸ³è¯­è°ƒã€æƒ…æ„Ÿç­‰ç‰¹å¾
    """
    try:
        if not audio.content_type.startswith("audio/"):
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒéŸ³é¢‘æ–‡ä»¶")
        
        audio_data = await audio.read()
        
        evaluator = get_multimodal_evaluator()
        voice_analysis = await evaluator.analyze_voice_characteristics(audio_data, transcript)
        
        return JSONResponse({
            "success": True,
            "voice_analysis": {
                "tone_confidence": round(voice_analysis.tone_confidence, 3),
                "speech_pace": voice_analysis.speech_pace,
                "volume_level": round(voice_analysis.volume_level, 3),
                "clarity": round(voice_analysis.clarity, 3),
                "emotional_tone": voice_analysis.emotional_tone,
                "pause_frequency": round(voice_analysis.pause_frequency, 3),
                "timestamp": voice_analysis.timestamp
            },
            "model_info": {
                "analyzer_model": "Doubao-Seed-1.6",
                "analysis_type": "voice_characteristics"
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³ç‰¹å¾åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@router.get("/model-capabilities")
async def get_model_capabilities():
    """
    è·å–Doubao-Seed-1.6æ¨¡å‹çš„èƒ½åŠ›ä¿¡æ¯
    """
    return JSONResponse({
        "model_name": "Doubao-Seed-1.6",
        "model_type": "multimodal",
        "capabilities": {
            "facial_analysis": {
                "emotions": ["confident", "nervous", "calm", "excited", "confused", "focused", "stressed"],
                "metrics": ["eye_contact", "smile_intensity", "attention_level", "confidence"]
            },
            "voice_analysis": {
                "characteristics": ["tone_confidence", "speech_pace", "volume_level", "clarity"],
                "emotional_tones": ["è‡ªä¿¡ä¸”ä¸“ä¸š", "ç´§å¼ ä½†åŠªåŠ›", "å¹³é™ç¨³å®š", "çƒ­æƒ…ç§¯æ"]
            },
            "content_analysis": {
                "dimensions": ["relevance", "completeness", "technical_accuracy", "communication_clarity"],
                "features": ["keyword_extraction", "quality_scoring", "relevance_assessment"]
            }
        },
        "integration_status": {
            "api_configured": True,
            "model_available": True,
            "fallback_enabled": True
        }
    })

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥é…ç½®
        config_valid = VITAConfig.get_config_summary()
        
        return JSONResponse({
            "status": "healthy",
            "model": "Doubao-Seed-1.6",
            "service": "multimodal_interview_api",
            "doubao_configured": config_valid.get("doubao_configured", False),
            "timestamp": time.time()
        })
        
    except Exception as e:
        return JSONResponse({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }, status_code=503) 