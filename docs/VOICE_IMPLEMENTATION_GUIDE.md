# VITA å®æ—¶è¯­éŸ³å¯¹è¯æ¨¡æ¿å®ç°æŒ‡å—

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†VITA (Virtual Interview & Training Assistant) é¡¹ç›®ä¸­å®æ—¶è¯­éŸ³å¯¹è¯åŠŸèƒ½çš„å®Œæ•´å®ç°ã€‚è¯¥åŠŸèƒ½é€šè¿‡é›†æˆOpenAIçš„Whisperå’ŒTTS APIï¼Œä¸ºç”¨æˆ·æä¾›çœŸå®çš„è¯­éŸ³é¢è¯•ä½“éªŒã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A[å‰ç«¯ç”¨æˆ·ç•Œé¢] --> B[åª’ä½“å½•åˆ¶å™¨]
    A --> C[éŸ³é¢‘æ’­æ”¾å™¨]
    B --> D[è¯­éŸ³æ•°æ®]
    D --> E[åç«¯è¯­éŸ³æœåŠ¡]
    E --> F[OpenAI Whisper API]
    E --> G[OpenAI TTS API]
    F --> H[æ–‡æœ¬è½¬å½•]
    G --> I[è¯­éŸ³åˆæˆ]
    H --> J[æ™ºèƒ½é¢è¯•åˆ†æ]
    I --> C
    J --> K[åé¦ˆç”Ÿæˆ]
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
VITA/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ speech.py              # è¯­éŸ³æœåŠ¡æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ main.py                    # APIç«¯ç‚¹ (å«è¯­éŸ³æ¥å£)
â”‚   â””â”€â”€ requirements.txt           # Pythonä¾èµ–
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceInterviewer.tsx     # è¯­éŸ³äº¤äº’ç»„ä»¶
â”‚   â”‚   â”‚   â””â”€â”€ VoiceInterviewDemo.tsx   # è¯­éŸ³åŠŸèƒ½æ¼”ç¤º
â”‚   â”‚   â””â”€â”€ store/
â”‚   â”‚       â””â”€â”€ useInterviewStore.ts      # çŠ¶æ€ç®¡ç†
â”‚   â””â”€â”€ package.json               # Node.jsä¾èµ–
â”œâ”€â”€ run_services.sh               # Linux/macOSå¯åŠ¨è„šæœ¬
â”œâ”€â”€ run_services.bat             # Windowså¯åŠ¨è„šæœ¬
â””â”€â”€ VOICE_FEATURES.md            # è¯­éŸ³åŠŸèƒ½æ–‡æ¡£
```

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. åç«¯è¯­éŸ³æœåŠ¡ (`backend/core/speech.py`)

```python
class SpeechService:
    """æ ¸å¿ƒè¯­éŸ³æœåŠ¡ç±»"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.supported_formats = ["mp3", "mp4", "mpeg", "mpga", "m4a", "wav", "webm"]
    
    async def speech_to_text(self, audio_data: bytes, language: str = "zh") -> Dict[str, Any]:
        """è¯­éŸ³è½¬æ–‡å­— - ä½¿ç”¨OpenAI Whisper"""
        # å®ç°è¯­éŸ³è¯†åˆ«é€»è¾‘
        
    async def text_to_speech(self, text: str, voice: str = "nova") -> bytes:
        """æ–‡å­—è½¬è¯­éŸ³ - ä½¿ç”¨OpenAI TTS"""
        # å®ç°è¯­éŸ³åˆæˆé€»è¾‘
        
    async def analyze_speech_features(self, audio_data: bytes) -> Dict[str, Any]:
        """åˆ†æè¯­éŸ³ç‰¹å¾ - è¯­é€Ÿã€åœé¡¿ã€æµç•…åº¦ç­‰"""
        # å®ç°è¯­éŸ³åˆ†æé€»è¾‘
```

### 2. APIç«¯ç‚¹è®¾è®¡ (`backend/main.py`)

```python
# è¯­éŸ³è½¬å½•
@app.post("/speech/transcribe")
async def transcribe_audio(audio: UploadFile, language: str = "zh")

# è¯­éŸ³åˆæˆ  
@app.post("/speech/synthesize")
async def synthesize_speech(text: str, voice: str = "nova")

# è¯­éŸ³åˆ†æ
@app.post("/speech/analyze")
async def analyze_speech(audio: UploadFile)

# é¢è¯•é—®é¢˜è¯­éŸ³
@app.post("/session/{session_id}/question/audio")
async def get_question_audio(session_id: str)

# è¯­éŸ³å›ç­”æäº¤
@app.post("/session/{session_id}/answer/voice")
async def submit_voice_answer(session_id: str, audio: UploadFile)
```

### 3. å‰ç«¯è¯­éŸ³ç»„ä»¶ (`frontend/src/components/VoiceInterviewer.tsx`)

```typescript
const VoiceInterviewer: React.FC<VoiceInterviewerProps> = ({
  question,
  onTranscription,
  isVoiceMode,
  onVoiceModeChange
}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  
  // è¯­éŸ³å½•åˆ¶åŠŸèƒ½
  const startVoiceRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const recorder = new MediaRecorder(stream);
    // å½•åˆ¶é€»è¾‘å®ç°
  };
  
  // è¯­éŸ³æ’­æ”¾åŠŸèƒ½
  const playQuestionAudio = async () => {
    const response = await fetch('/speech/synthesize', {
      method: 'POST',
      body: new URLSearchParams({ text: question, voice: 'nova' })
    });
    // æ’­æ”¾é€»è¾‘å®ç°
  };
  
  return (
    // UIç»„ä»¶æ¸²æŸ“
  );
};
```

## ğŸ¤ è¯­éŸ³äº¤äº’æµç¨‹

### æ ‡å‡†é¢è¯•å¯¹è¯æµç¨‹

1. **é¢è¯•å¼€å§‹**
   ```
   ç³»ç»Ÿ: "ä½ å¥½ï¼Œæ¬¢è¿å‚åŠ é¢è¯•ã€‚æˆ‘æ˜¯AIé¢è¯•å®˜ï¼Œä»Šå¤©å°†ä¸ºæ‚¨è¿›è¡Œæ¨¡æ‹Ÿé¢è¯•ã€‚"
   ```

2. **é—®é¢˜æå‡º**
   ```
   AIé¢è¯•å®˜: "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æ‚¨çš„å·¥ä½œç»éªŒå’ŒæŠ€æœ¯èƒŒæ™¯ã€‚"
   (è‡ªåŠ¨è¯­éŸ³æ’­æ”¾)
   ```

3. **ç”¨æˆ·å›ç­”**
   ```
   ç”¨æˆ·: [ç‚¹å‡»å½•éŸ³æŒ‰é’®] 
   "æˆ‘æœ‰5å¹´çš„è½¯ä»¶å¼€å‘ç»éªŒï¼Œä¸»è¦ä½¿ç”¨Pythonå’ŒJavaScript..."
   [åœæ­¢å½•éŸ³ï¼Œè‡ªåŠ¨è½¬å½•]
   ```

4. **æ™ºèƒ½è¿½é—®**
   ```
   AIé¢è¯•å®˜: "æ‚¨æåˆ°äº†Pythonå¼€å‘ç»éªŒï¼Œèƒ½å…·ä½“è¯´è¯´æ‚¨å‚ä¸è¿‡çš„é¡¹ç›®å—ï¼Ÿ"
   ```

5. **é¢è¯•ç»“æŸ**
   ```
   AIé¢è¯•å®˜: "æ„Ÿè°¢æ‚¨çš„å›ç­”ã€‚é¢è¯•åˆ°æ­¤ç»“æŸï¼Œç¨åå°†ä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†çš„åé¦ˆæŠ¥å‘Šã€‚"
   ```

### è¯­éŸ³åˆ†æç»´åº¦

- **å†…å®¹è´¨é‡** (60%)
  - å›ç­”å®Œæ•´æ€§
  - é€»è¾‘æ¸…æ™°åº¦
  - ä¸“ä¸šæœ¯è¯­ä½¿ç”¨

- **è¯­éŸ³è¡¨ç°** (40%)
  - è¯­é€Ÿæ§åˆ¶ (120-180 WPMä¸ºä½³)
  - åœé¡¿åˆç†æ€§ (<3ç§’åœé¡¿)
  - å‘éŸ³æ¸…æ™°åº¦
  - æµç•…åº¦è¯„åˆ†

## ğŸ› ï¸ å¼€å‘ç¯å¢ƒè®¾ç½®

### 1. ç¯å¢ƒè¦æ±‚

```bash
# Python ç¯å¢ƒ
Python 3.8+
OpenAI API Key

# Node.js ç¯å¢ƒ  
Node.js 16+
npm æˆ– yarn

# æµè§ˆå™¨æ”¯æŒ
Chrome 70+
Firefox 65+
Safari 14+
Edge 80+
```

### 2. å¿«é€Ÿå¯åŠ¨

```bash
# å…‹éš†é¡¹ç›®
git clone <repo-url>
cd VITA

# è®¾ç½®ç¯å¢ƒå˜é‡
export # OpenAIé…ç½®å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨æœ¬åœ°Whisper + æœ¬åœ°TTS

# å¯åŠ¨æ‰€æœ‰æœåŠ¡ (Linux/macOS)
chmod +x run_services.sh
./run_services.sh

# æˆ–å¯åŠ¨æ‰€æœ‰æœåŠ¡ (Windows)
run_services.bat
```

### 3. å¼€å‘è°ƒè¯•

```bash
# åç«¯æœåŠ¡
cd backend
python -m uvicorn main:app --reload --port 8000

# å‰ç«¯æœåŠ¡
cd frontend  
npm run dev

# æŸ¥çœ‹APIæ–‡æ¡£
http://localhost:8000/docs
```

## ğŸ§ª åŠŸèƒ½æµ‹è¯•

### 1. è¯­éŸ³åˆæˆæµ‹è¯•

```bash
curl -X POST "http://localhost:8000/speech/synthesize" \
  -F "text=ä½ å¥½ï¼Œæ¬¢è¿å‚åŠ VITAè™šæ‹Ÿé¢è¯•" \
  -F "voice=nova" \
  -F "speed=1.0" \
  --output test_audio.mp3
```

### 2. è¯­éŸ³è¯†åˆ«æµ‹è¯•

```bash
curl -X POST "http://localhost:8000/speech/transcribe" \
  -F "audio=@test_recording.webm" \
  -F "language=zh"
```

### 3. å‰ç«¯æ¼”ç¤ºé¡µé¢

è®¿é—® `http://localhost:5173/voice-demo` è¿›è¡Œå®Œæ•´çš„è¯­éŸ³åŠŸèƒ½æµ‹è¯•ã€‚

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### 1. éŸ³é¢‘å¤„ç†ä¼˜åŒ–

```typescript
// éŸ³é¢‘æ ¼å¼ä¼˜åŒ–
const mediaRecorder = new MediaRecorder(stream, {
  mimeType: 'audio/webm;codecs=opus',
  audioBitsPerSecond: 64000  // é™ä½ç ç‡ä»¥æå‡ä¼ è¾“é€Ÿåº¦
});

// åˆ†å—ä¸Šä¼ å¤§éŸ³é¢‘æ–‡ä»¶
const chunkSize = 1024 * 1024; // 1MB chunks
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# è¯­éŸ³åˆæˆç»“æœç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
async def cached_text_to_speech(text: str, voice: str) -> bytes:
    return await speech_service.text_to_speech(text, voice)
```

### 3. å¹¶å‘å¤„ç†

```python
# å¼‚æ­¥å¤„ç†å¤šä¸ªè¯­éŸ³è¯·æ±‚
import asyncio

async def process_multiple_audios(audio_files: list):
    tasks = [speech_service.speech_to_text(audio) for audio in audio_files]
    return await asyncio.gather(*tasks)
```

## ğŸ” å®‰å…¨è€ƒè™‘

### 1. æ•°æ®ä¿æŠ¤

```python
# éŸ³é¢‘æ–‡ä»¶ä¸´æ—¶å­˜å‚¨ï¼Œå¤„ç†åç«‹å³åˆ é™¤
with tempfile.NamedTemporaryFile(delete=True) as temp_file:
    temp_file.write(audio_data)
    result = await process_audio(temp_file.name)
    # æ–‡ä»¶è‡ªåŠ¨åˆ é™¤
```

### 2. æƒé™æ§åˆ¶

```typescript
// éº¦å…‹é£æƒé™æ£€æŸ¥
const checkMicrophonePermission = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    stream.getTracks().forEach(track => track.stop());
    return true;
  } catch (error) {
    console.error('éº¦å…‹é£æƒé™è¢«æ‹’ç»:', error);
    return false;
  }
};
```

### 3. è¾“å…¥éªŒè¯

```python
# éŸ³é¢‘æ–‡ä»¶éªŒè¯
async def validate_audio_file(file: UploadFile):
    if not file.content_type.startswith('audio/'):
        raise HTTPException(400, "æ–‡ä»¶å¿…é¡»æ˜¯éŸ³é¢‘æ ¼å¼")
    
    if file.size > 25 * 1024 * 1024:  # 25MB limit
        raise HTTPException(400, "éŸ³é¢‘æ–‡ä»¶è¿‡å¤§")
```

## ğŸ“Š ç›‘æ§ä¸åˆ†æ

### 1. æ€§èƒ½ç›‘æ§

```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start_time
        print(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        return result
    return wrapper

@monitor_performance
async def speech_to_text(self, audio_data: bytes):
    # å®ç°é€»è¾‘
```

### 2. é”™è¯¯æ—¥å¿—

```python
import logging

logger = logging.getLogger(__name__)

try:
    result = await speech_service.speech_to_text(audio_data)
except Exception as e:
    logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}", extra={
        'audio_size': len(audio_data),
        'user_id': user_id,
        'timestamp': time.time()
    })
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
# docker-compose.yml
version: '3.8'
services:
  vita-backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - # OpenAIé…ç½®å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨æœ¬åœ°Whisper + æœ¬åœ°TTS
    
  vita-frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - vita-backend
```

### 2. æ€§èƒ½è°ƒä¼˜

```bash
# å¢åŠ æ–‡ä»¶ä¸Šä¼ é™åˆ¶
# nginx.conf
client_max_body_size 50M;

# å¯ç”¨gzipå‹ç¼©
gzip on;
gzip_types audio/webm audio/mpeg;
```

### 3. ç›‘æ§è®¾ç½®

```bash
# ä½¿ç”¨PM2ç®¡ç†Node.jsè¿›ç¨‹
npm install -g pm2
pm2 start ecosystem.config.js

# ä½¿ç”¨Supervisorç®¡ç†Pythonè¿›ç¨‹
supervisord -c supervisord.conf
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. ç”¨æˆ·ä½“éªŒ

- **æ¸è¿›å¼æƒé™è¯·æ±‚**ï¼šåªåœ¨éœ€è¦æ—¶è¯·æ±‚éº¦å…‹é£æƒé™
- **å®æ—¶åé¦ˆ**ï¼šæ˜¾ç¤ºå½•éŸ³çŠ¶æ€å’ŒéŸ³é‡æŒ‡ç¤º
- **ä¼˜é›…é™çº§**ï¼šè¯­éŸ³åŠŸèƒ½å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡å­—æ¨¡å¼
- **å“åº”å¼è®¾è®¡**ï¼šé€‚é…å„ç§è®¾å¤‡å±å¹•

### 2. æ€§èƒ½ä¼˜åŒ–

- **éŸ³é¢‘å‹ç¼©**ï¼šä½¿ç”¨åˆé€‚çš„ç¼–è§£ç å™¨
- **è¯·æ±‚ç¼“å­˜**ï¼šç¼“å­˜å¸¸ç”¨çš„è¯­éŸ³åˆæˆç»“æœ
- **åˆ†å—ä¼ è¾“**ï¼šå¤§æ–‡ä»¶åˆ†å—ä¸Šä¼ 
- **è¿æ¥æ± **ï¼šå¤ç”¨HTTPè¿æ¥

### 3. é”™è¯¯å¤„ç†

- **ç½‘ç»œå¼‚å¸¸**ï¼šè‡ªåŠ¨é‡è¯•æœºåˆ¶
- **APIé™åˆ¶**ï¼šè¯·æ±‚é€Ÿç‡æ§åˆ¶
- **æƒé™æ‹’ç»**ï¼šå‹å¥½çš„é”™è¯¯æç¤º
- **æ ¼å¼ä¸æ”¯æŒ**ï¼šè‡ªåŠ¨æ ¼å¼è½¬æ¢

## ğŸ“ˆ åŠŸèƒ½æ‰©å±•è·¯çº¿å›¾

### Phase 1: åŸºç¡€å®ç° (å·²å®Œæˆ)
- âœ… è¯­éŸ³è½¬æ–‡å­— (Whisper)
- âœ… æ–‡å­—è½¬è¯­éŸ³ (TTS)
- âœ… åŸºç¡€è¯­éŸ³åˆ†æ
- âœ… å‰ç«¯UIç»„ä»¶

### Phase 2: é«˜çº§åŠŸèƒ½ (å¼€å‘ä¸­)
- ğŸ”„ å®æ—¶è¯­éŸ³æµå¤„ç†
- ğŸ”„ å¤šè¯­è¨€æ”¯æŒæ‰©å±•
- ğŸ”„ æƒ…ç»ªè¯†åˆ«é›†æˆ
- ğŸ”„ è¯­éŸ³è´¨é‡è¯„ä¼°

### Phase 3: æ™ºèƒ½ä¼˜åŒ– (è§„åˆ’ä¸­)
- ğŸ“‹ è‡ªé€‚åº”è¯­éŸ³è¯†åˆ«
- ğŸ“‹ ä¸ªæ€§åŒ–è¯­éŸ³åˆæˆ
- ğŸ“‹ å®æ—¶è¯­éŸ³æ•™ç»ƒ
- ğŸ“‹ 3Dè™šæ‹Ÿé¢è¯•å®˜

## ğŸ’¡ åˆ›æ–°äº®ç‚¹

1. **å¤šæ¨¡æ€äº¤äº’**ï¼šè¯­éŸ³+è§†è§‰+æ–‡æœ¬çš„ç»¼åˆåˆ†æ
2. **æ™ºèƒ½é¢è¯•å®˜**ï¼šåŸºäºGPT-4çš„åŠ¨æ€é—®é¢˜ç”Ÿæˆ
3. **å®æ—¶åé¦ˆ**ï¼šå³æ—¶çš„è¯­éŸ³è¡¨ç°åˆ†æ
4. **ä¸ªæ€§åŒ–ä½“éªŒ**ï¼šå¯å®šåˆ¶çš„è¯­éŸ³é£æ ¼å’Œé¢è¯•åœºæ™¯

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæ”¹è¿›å»ºè®®ï¼

1. Fork é¡¹ç›®ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/voice-enhancement`)
3. æäº¤æ›´æ”¹ (`git commit -am 'Add voice enhancement'`)
4. æ¨é€åˆ†æ”¯ (`git push origin feature/voice-enhancement`)
5. åˆ›å»º Pull Request

## ğŸ“ æŠ€æœ¯æ”¯æŒ

- ğŸ“§ Email: support@vita-ai.com
- ğŸ’¬ Discord: [VITAå¼€å‘è€…ç¤¾åŒº]
- ğŸ“š æ–‡æ¡£: https://docs.vita-ai.com
- ğŸ› é—®é¢˜åé¦ˆ: GitHub Issues

---

**é€šè¿‡VITAçš„å®æ—¶è¯­éŸ³å¯¹è¯æ¨¡æ¿ï¼Œè®©æ¯ä¸€æ¬¡é¢è¯•ç»ƒä¹ éƒ½æ›´åŠ çœŸå®æœ‰æ•ˆï¼** ğŸ¯ğŸ™ï¸ 