# VITA åŒAPIæ¶æ„è¯¦è§£

## ğŸŒ æ¶æ„æ¦‚è¿°

VITAé‡‡ç”¨**Llama API + Qwen API**çš„åŒäº‘ç«¯APIæ¶æ„ï¼Œå®ç°é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ™ºèƒ½é¢è¯•ç³»ç»Ÿï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Llama API          â”‚    â”‚       Qwen API          â”‚
â”‚  api.llama-api.com      â”‚    â”‚ dashscope.aliyuncs.com  â”‚
â”‚  (ç¬¬ä¸‰æ–¹APIæœåŠ¡)        â”‚    â”‚    (é˜¿é‡Œäº‘API)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ APIåˆ‡æ¢ç®¡ç†å™¨   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   VITA åº”ç”¨    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ APIé…ç½®è¯¦æƒ…

### 1. Llama API (ä¸»åŠ›)
- **æä¾›å•†**: ç¬¬ä¸‰æ–¹Llama APIæœåŠ¡
- **APIåœ°å€**: `https://api.llama-api.com/v1`
- **è®¤è¯æ ¼å¼**: `LLM|æ•°å­—ID|å¯†é’¥å­—ç¬¦ä¸²`
- **ç¤ºä¾‹å¯†é’¥**: `LLM|727268019715816|R9EX2i7cmHya1_7HAFiIAxxtAUk`

**å¯ç”¨æ¨¡å‹**:
```python
LLAMA_MODELS = {
    "chat": "Llama-3.3-70B-Instruct",           # ä¸»è¦å¯¹è¯æ¨¡å‹
    "analysis": "__REMOVED_API_KEY__",  # æ·±åº¦åˆ†æ
    "interview": "Llama-3.3-70B-Instruct",      # é¢è¯•ä¸“ç”¨
    "code": "__REMOVED_API_KEY__",  # ä»£ç ç›¸å…³
    "math": "__REMOVED_API_KEY__",  # æ•°å­¦æ¨ç†
    "fallback": "Llama-3.3-8B-Instruct"        # è½»é‡å¤‡ç”¨
}
```

### 2. Qwen API (å¤‡ä»½/ç‰¹æ®Šä»»åŠ¡)
- **æä¾›å•†**: é˜¿é‡Œäº‘DashScope
- **APIåœ°å€**: `https://dashscope.aliyuncs.com/compatible-mode/v1`
- **è®¤è¯æ ¼å¼**: `sk-` å¼€å¤´çš„çŸ­å¯†é’¥
- **ç¤ºä¾‹å¯†é’¥**: `__REMOVED_API_KEY__`

**å¯ç”¨æ¨¡å‹**:
```python
QWEN_MODELS = {
    "chat": "qwen-plus",                        # é€šç”¨å¯¹è¯
    "analysis": "qwen-plus",                    # å†…å®¹åˆ†æ
    "turbo": "qwen-turbo",                     # å¿«é€Ÿå“åº”
    "long": "qwen-long",                       # é•¿æ–‡æœ¬å¤„ç†
    "audio": "Qwen/Qwen2-Audio-7B-Instruct",  # éŸ³é¢‘å¤„ç†
    "code": "Qwen/Qwen2.5-Coder-32B-Instruct",    # ä»£ç ä»»åŠ¡
    "math": "Qwen/Qwen2.5-Math-72B-Instruct", # æ•°å­¦æ¨ç†
    "vision": "Qwen/Qwen2-VL-72B-Instruct"    # è§†è§‰ç†è§£
}
```

## ğŸ”„ æ™ºèƒ½åˆ‡æ¢æœºåˆ¶

### åˆ‡æ¢ç­–ç•¥
1. **ä»»åŠ¡ä¼˜å…ˆ**: éŸ³é¢‘ä»»åŠ¡â†’Qwen-Audio, è§†è§‰ä»»åŠ¡â†’Qwen-VL
2. **å¥åº·çŠ¶æ€**: ä¸»APIæ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨API
3. **æ€§èƒ½ä¼˜åŒ–**: æ ¹æ®å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´
4. **è´Ÿè½½å‡è¡¡**: åˆ†æ•£è¯·æ±‚å‹åŠ›

### åˆ‡æ¢è§¦å‘æ¡ä»¶
```python
# è‡ªåŠ¨åˆ‡æ¢æ¡ä»¶
SWITCH_CONDITIONS = {
    "api_failure": True,        # APIæœåŠ¡å¤±è´¥
    "timeout": 30,              # å“åº”è¶…æ—¶(ç§’)
    "error_rate": 0.1,          # é”™è¯¯ç‡é˜ˆå€¼
    "rate_limit": True,         # è¾¾åˆ°é€Ÿç‡é™åˆ¶
    "model_unavailable": True   # æ¨¡å‹ä¸å¯ç”¨
}
```

## ğŸ“ ç¯å¢ƒé…ç½®

### åŸºç¡€é…ç½® (.env)
```env
# === Llama APIé…ç½® ===
LLAMA_API_KEY=LLM|727268019715816|R9EX2i7cmHya1_7HAFiIAxxtAUk
LLAMA_API_BASE_URL=https://api.llama-api.com/v1

# === Qwen APIé…ç½® ===  
QWEN_API_KEY=__REMOVED_API_KEY__

# === åˆ‡æ¢æ§åˆ¶ ===
USE_QWEN_FALLBACK=true        # å¯ç”¨Qwenå¤‡ä»½
PREFER_LLAMA=true             # ä¼˜å…ˆä½¿ç”¨Llama
ENABLE_AUTO_SWITCH=true       # å¯ç”¨è‡ªåŠ¨åˆ‡æ¢
HEALTH_CHECK_INTERVAL=60      # å¥åº·æ£€æŸ¥é—´éš”(ç§’)
MAX_RETRY_COUNT=3             # æœ€å¤§é‡è¯•æ¬¡æ•°

# === æ€§èƒ½é…ç½® ===
REQUEST_TIMEOUT=30            # è¯·æ±‚è¶…æ—¶
MAX_CONCURRENT_REQUESTS=10    # æœ€å¤§å¹¶å‘æ•°
RATE_LIMIT_PER_MINUTE=60     # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
```

### Pythonå®¢æˆ·ç«¯é…ç½®
```python
# å®¢æˆ·ç«¯åˆå§‹åŒ–ç¤ºä¾‹
from core.openai_compat import create_openai_client

# Llamaå®¢æˆ·ç«¯
llama_client = create_openai_client(
    api_key="LLM|727268019715816|R9EX2i7cmHya1_7HAFiIAxxtAUk",
    base_url="https://api.llama-api.com/v1"
)

# Qwenå®¢æˆ·ç«¯  
qwen_client = create_openai_client(
    api_key="__REMOVED_API_KEY__"
)
```

## ğŸš€ APIè°ƒç”¨ç¤ºä¾‹

### 1. èŠå¤©å¯¹è¯
```python
# ä½¿ç”¨Llamaè¿›è¡Œé¢è¯•å¯¹è¯
response = await llama_client.chat.completions.create(
    model="Llama-3.3-70B-Instruct",
    messages=[
        {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„å·¥ä½œç»éªŒ"}
    ],
    max_tokens=500
)

# è‡ªåŠ¨åˆ‡æ¢åˆ°Qwenï¼ˆå¦‚æœLlamaä¸å¯ç”¨ï¼‰
response = await qwen_client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„å·¥ä½œç»éªŒ"}
    ],
    max_tokens=500
)
```

### 2. éŸ³é¢‘å¤„ç†
```python
# ä½¿ç”¨Qwenè¿›è¡Œè¯­éŸ³è¯†åˆ«
response = await qwen_client.audio.transcriptions.create(
    model="Qwen/Qwen2-Audio-7B-Instruct",
    file=audio_file,
    response_format="json"
)
```

### 3. å¥åº·æ£€æŸ¥
```python
# APIå¥åº·çŠ¶æ€æ£€æŸ¥
async def check_api_health(client, model):
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Hi"}],
            max_tokens=1,
            timeout=10
        )
        return True
    except:
        return False
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### å®æ—¶ç›‘æ§ç«¯ç‚¹
- `GET /api/v1/system/status` - ç³»ç»ŸçŠ¶æ€
- `GET /api/v1/system/performance` - æ€§èƒ½æŒ‡æ ‡  
- `GET /api/v1/system/switch-status` - åˆ‡æ¢çŠ¶æ€
- `POST /api/v1/system/switch-primary` - æ‰‹åŠ¨åˆ‡æ¢

### å…³é”®æŒ‡æ ‡
```json
{
  "api_status": {
    "llama": {
      "status": "healthy",
      "response_time_ms": 800,
      "success_rate": 99.5,
      "requests_today": 1250,
      "last_error": null
    },
    "qwen": {
      "status": "healthy", 
      "response_time_ms": 600,
      "success_rate": 99.8,
      "requests_today": 340,
      "last_error": null
    }
  },
  "switch_count_today": 3,
  "current_primary": "llama",
  "auto_switch_enabled": true
}
```

## ğŸ’° æˆæœ¬ç®¡ç†

### æˆæœ¬ç›‘æ§
```python
# æˆæœ¬è·Ÿè¸ª
COST_TRACKING = {
    "llama": {
        "input_tokens": 0.001,    # æ¯1K tokensæˆæœ¬
        "output_tokens": 0.002,   # æ¯1K tokensæˆæœ¬
        "requests_today": 1250,
        "cost_today": 15.60
    },
    "qwen": {
        "input_tokens": 0.0008,   # æ¯1K tokensæˆæœ¬
        "output_tokens": 0.002,   # æ¯1K tokensæˆæœ¬  
        "requests_today": 340,
        "cost_today": 4.20
    }
}
```

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥
1. **ç¼“å­˜æœºåˆ¶**: é‡å¤è¯·æ±‚ä½¿ç”¨ç¼“å­˜ç»“æœ
2. **æ¨¡å‹é€‰æ‹©**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚æ¨¡å‹
3. **è¯·æ±‚ä¼˜åŒ–**: å‡å°‘ä¸å¿…è¦çš„tokenä½¿ç”¨
4. **é¢„ç®—æ§åˆ¶**: è®¾ç½®æ¯æ—¥æ¶ˆè´¹é™é¢

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†

### è‡ªåŠ¨é‡è¯•æœºåˆ¶
```python
# é‡è¯•é…ç½®
RETRY_CONFIG = {
    "max_retries": 3,
    "base_delay": 1.0,
    "exponential_backoff": True,
    "retry_on": [
        "connection_error",
        "timeout",
        "rate_limit",
        "server_error"
    ]
}
```

### æ•…éšœè½¬ç§»æµç¨‹
1. **æ£€æµ‹æ•…éšœ**: APIå¥åº·æ£€æŸ¥å¤±è´¥
2. **å°è¯•é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•
3. **åˆ‡æ¢API**: è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨API
4. **è®°å½•æ—¥å¿—**: è¯¦ç»†è®°å½•åˆ‡æ¢åŸå› 
5. **æ¢å¤æ£€æµ‹**: å®šæœŸæ£€æŸ¥åŸAPIæ¢å¤çŠ¶æ€

## ğŸ”— APIæ–‡æ¡£é“¾æ¥

- [Llama APIæ–‡æ¡£](https://docs.llama-api.com/) (å‡è®¾é“¾æ¥)
- [é˜¿é‡Œäº‘DashScopeæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [OpenAIå…¼å®¹æ ¼å¼](https://platform.openai.com/docs/api-reference)

---

**VITAçš„åŒAPIæ¶æ„ç¡®ä¿äº†é«˜å¯ç”¨æ€§ã€é«˜æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šçš„å®Œç¾å¹³è¡¡ï¼** ğŸš€ 