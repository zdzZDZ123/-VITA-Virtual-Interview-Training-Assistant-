import React, { useState, useRef, useEffect, useCallback } from 'react';
import { useInterviewStore } from '../store/useInterviewStore';
import { toast } from 'react-hot-toast';

interface VoiceMessage {
  id: string;
  role: 'interviewer' | 'candidate';
  content: string;
  timestamp: Date;
  audioUrl?: string;
  isPlaying?: boolean;
  duration?: number;
}

interface VoiceState {
  isListening: boolean;
  isProcessing: boolean;
  isSpeaking: boolean;
  isConnected: boolean;
  volume: number;
  confidence: number;
}

const RealTimeVoiceInterview: React.FC = () => {
  const { sessionId, currentQuestion, submitAnswer, isLoading } = useInterviewStore();
  
  // æ·»åŠ è°ƒè¯•æ¨¡å¼çŠ¶æ€
  const [debugMode, setDebugMode] = useState(false);
  
  // è¯­éŸ³çŠ¶æ€
  const [voiceState, setVoiceState] = useState<VoiceState>({
    isListening: false,
    isProcessing: false,
    isSpeaking: false,
    isConnected: false,
    volume: 0,
    confidence: 0
  });
  
  // å¯¹è¯å†å²
  const [messages, setMessages] = useState<VoiceMessage[]>([]);
  
  // å½“å‰ç”¨æˆ·è¾“å…¥
  const [currentInput, setCurrentInput] = useState('');
  const [isRecording, setIsRecording] = useState(false);
  
  // è¯­éŸ³è®¾ç½®
  const [voiceSettings, setVoiceSettings] = useState({
    voice: 'nova',
    speed: 1.0,
    autoPlay: true,
    interruptible: true,
    silenceThreshold: 2000, // 2ç§’é™é»˜ååœæ­¢å½•éŸ³
    volumeThreshold: 0.01
  });
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioRef = useRef<HTMLAudioElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  
  // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
  const initializeAudioContext = useCallback(async () => {
    try {
      console.log('[RealTimeVoiceInterview:Audio] å¼€å§‹åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡...');
      
      // å¦‚æœå·²æœ‰AudioContextä¸”çŠ¶æ€æ­£å¸¸ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        if (audioContextRef.current.state === 'suspended') {
          console.log('[RealTimeVoiceInterview:Audio] ç°æœ‰éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„äºæŒ‚èµ·çŠ¶æ€ï¼Œæ­£åœ¨æ¢å¤...');
          await audioContextRef.current.resume();
          console.log('[RealTimeVoiceInterview:Audio] ç°æœ‰éŸ³é¢‘ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸ');
        }
        console.log('[RealTimeVoiceInterview:Audio] ä½¿ç”¨ç°æœ‰éŸ³é¢‘ä¸Šä¸‹æ–‡ï¼ŒçŠ¶æ€:', audioContextRef.current.state);
        return true;
      }
      
      // åˆ›å»ºæ–°çš„AudioContext
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ');
      
      if (audioContextRef.current.state === 'suspended') {
        console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„äºæŒ‚èµ·çŠ¶æ€ï¼Œæ­£åœ¨æ¢å¤...');
        await audioContextRef.current.resume();
        console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸ');
      }
      
      console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘ä¸Šä¸‹æ–‡åˆå§‹åŒ–æˆåŠŸï¼ŒçŠ¶æ€:', audioContextRef.current.state);
      return true;
    } catch (error) {
      console.error('[RealTimeVoiceInterview:Audio] éŸ³é¢‘ä¸Šä¸‹æ–‡åˆå§‹åŒ–å¤±è´¥:', error);
      console.error('[RealTimeVoiceInterview:Audio] é”™è¯¯è¯¦æƒ…:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      return false;
    }
  }, []);
  
  // éŸ³é‡æ£€æµ‹
  const analyzeAudio = useCallback(() => {
    if (!analyserRef.current) return;
    
    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    // è®¡ç®—éŸ³é‡
    const volume = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length / 255;
    
    setVoiceState(prev => ({ ...prev, volume }));
    
    // é™é»˜æ£€æµ‹
    if (volume < voiceSettings.volumeThreshold) {
      if (!silenceTimerRef.current && isRecording) {
        silenceTimerRef.current = setTimeout(() => {
          stopListening();
        }, voiceSettings.silenceThreshold);
      }
    } else {
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    }
    
    if (isRecording) {
      animationFrameRef.current = requestAnimationFrame(analyzeAudio);
    }
  }, [isRecording, voiceSettings.volumeThreshold, voiceSettings.silenceThreshold]);
  
  // å¼€å§‹ç›‘å¬
  const startListening = useCallback(async () => {
    try {
      console.log('[RealTimeVoiceInterview:Audio] å¼€å§‹ç›‘å¬æµç¨‹...');
      
      const audioInitialized = await initializeAudioContext();
      if (!audioInitialized) {
        throw new Error('éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥');
      }
      
      console.log('[RealTimeVoiceInterview:Audio] æ­£åœ¨è¯·æ±‚éº¦å…‹é£æƒé™...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        }
      });
      
      console.log('[RealTimeVoiceInterview:Audio] æˆåŠŸè·å–åª’ä½“æµ:', {
        streamId: stream.id,
        active: stream.active,
        audioTracks: stream.getAudioTracks().length
      });
      
      streamRef.current = stream;
      
      // è®¾ç½®éŸ³é¢‘åˆ†æ
      const audioContext = audioContextRef.current!;
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      source.connect(analyser);
      analyserRef.current = analyser;
      
      console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘åˆ†æå™¨å·²è®¾ç½®');
      
      // è®¾ç½®å½•éŸ³ - æ£€æµ‹æ”¯æŒçš„MIMEç±»å‹
      let mimeType = 'audio/webm;codecs=opus';
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = 'audio/webm';
        if (!MediaRecorder.isTypeSupported(mimeType)) {
          mimeType = 'audio/mp4';
          if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = ''; // ä½¿ç”¨é»˜è®¤æ ¼å¼
          }
        }
      }
      
      console.log('[RealTimeVoiceInterview:Audio] ä½¿ç”¨MIMEç±»å‹:', mimeType || 'é»˜è®¤');
      
      const mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : undefined);
      
      audioChunksRef.current = [];
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘æ•°æ®å¯ç”¨ï¼Œå¤§å°:', event.data.size);
        } else {
          console.warn('[RealTimeVoiceInterview:Audio] æ”¶åˆ°ç©ºçš„éŸ³é¢‘æ•°æ®å—');
        }
      };
      
      mediaRecorder.onstop = async () => {
        console.log('[RealTimeVoiceInterview:Audio] å½•éŸ³åœæ­¢ï¼Œå‡†å¤‡å¤„ç†éŸ³é¢‘...');
        console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘å—æ€»æ•°:', audioChunksRef.current.length);
        
        if (audioChunksRef.current.length === 0) {
          console.error('[RealTimeVoiceInterview:Audio] æ²¡æœ‰æ”¶é›†åˆ°éŸ³é¢‘æ•°æ®');
          toast.error('å½•éŸ³å¤±è´¥ï¼šæ²¡æœ‰æ”¶é›†åˆ°éŸ³é¢‘æ•°æ®ï¼Œè¯·é‡è¯•');
          return;
        }
        
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType || 'audio/webm' });
        console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘Blobåˆ›å»ºå®Œæˆï¼Œå¤§å°:', audioBlob.size);
        
        if (audioBlob.size === 0) {
          console.error('[RealTimeVoiceInterview:Audio] éŸ³é¢‘Blobä¸ºç©º');
          toast.error('å½•éŸ³å¤±è´¥ï¼šéŸ³é¢‘æ•°æ®ä¸ºç©ºï¼Œè¯·é‡è¯•');
          return;
        }
        
        await processAudioInput(audioBlob);
      };
      
      mediaRecorder.onstart = () => {
        console.log('[RealTimeVoiceInterview:Audio] MediaRecorderå·²å¯åŠ¨');
      };
      
      mediaRecorder.onerror = (event) => {
        console.error('[RealTimeVoiceInterview:Audio] MediaRecorderé”™è¯¯:', event);
        toast.error('å½•éŸ³å™¨é”™è¯¯ï¼Œè¯·é‡è¯•');
      };
      
      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start(1000); // æ¯1ç§’æ”¶é›†ä¸€æ¬¡æ•°æ®ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„åˆ†ç‰‡
      
      setIsRecording(true);
      setVoiceState(prev => ({ ...prev, isListening: true, isConnected: true }));
      
      console.log('[RealTimeVoiceInterview:Audio] å¼€å§‹å½•éŸ³ï¼Œæ¯100msæ”¶é›†ä¸€æ¬¡æ•°æ®');
      
      // å¼€å§‹éŸ³é‡åˆ†æ
      analyzeAudio();
      
      toast.success('å¼€å§‹ç›‘å¬ï¼Œè¯·è¯´è¯...');
      
    } catch (error) {
      console.error('[RealTimeVoiceInterview:Audio] å¼€å§‹ç›‘å¬å¤±è´¥:', error);
      console.error('[RealTimeVoiceInterview:Audio] é”™è¯¯è¯¦æƒ…:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      toast.error('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
      setVoiceState(prev => ({ ...prev, isListening: false, isConnected: false }));
    }
  }, [initializeAudioContext, analyzeAudio]);
  
  // åœæ­¢ç›‘å¬
  const stopListening = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    
    setVoiceState(prev => ({ ...prev, isListening: false, volume: 0 }));
  }, [isRecording]);
  
  // å¤„ç†éŸ³é¢‘è¾“å…¥
  const processAudioInput = useCallback(async (audioBlob: Blob) => {
    console.log('[RealTimeVoiceInterview:Audio] å¼€å§‹å¤„ç†éŸ³é¢‘è¾“å…¥ï¼ŒBlobå¤§å°:', audioBlob.size);
    setVoiceState(prev => ({ ...prev, isProcessing: true }));
    
    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');
      formData.append('language', 'zh');
      
      console.log('[RealTimeVoiceInterview:Audio] å‡†å¤‡å‘é€éŸ³é¢‘åˆ°åç«¯è¿›è¡Œè½¬å½•...');
      const response = await fetch('http://localhost:8000/speech/transcribe', {
        method: 'POST',
        body: formData
      });
      
      console.log('[RealTimeVoiceInterview:Audio] è½¬å½•å“åº”çŠ¶æ€:', response.status);
      
      const data = await response.json();
      console.log('[RealTimeVoiceInterview:Audio] è½¬å½•å“åº”æ•°æ®:', data);
      
      if (data.success && data.text.trim()) {
        const userMessage: VoiceMessage = {
          id: Date.now().toString(),
          role: 'candidate',
          content: data.text,
          timestamp: new Date(),
          duration: data.duration,
        };
        
        console.log('[RealTimeVoiceInterview:Audio] è½¬å½•æˆåŠŸï¼Œæ–‡æœ¬:', data.text);
        console.log('[RealTimeVoiceInterview:Audio] ç½®ä¿¡åº¦:', data.confidence);
        
        setMessages(prev => [...prev, userMessage]);
        setCurrentInput(data.text);
        setVoiceState(prev => ({ ...prev, confidence: data.confidence || 0 }));
        
        // æäº¤ç­”æ¡ˆå¹¶è·å–ä¸‹ä¸€ä¸ªé—®é¢˜
        await handleSubmitAnswer(data.text);
      } else {
        console.warn('[RealTimeVoiceInterview:Audio] è½¬å½•å¤±è´¥æˆ–æ–‡æœ¬ä¸ºç©º:', data);
      }
    } catch (error) {
      console.error('[RealTimeVoiceInterview:Audio] è¯­éŸ³å¤„ç†å¤±è´¥:', error);
      console.error('[RealTimeVoiceInterview:Audio] é”™è¯¯è¯¦æƒ…:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      toast.error('è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•');
    } finally {
      setVoiceState(prev => ({ ...prev, isProcessing: false }));
      console.log('[RealTimeVoiceInterview:Audio] éŸ³é¢‘å¤„ç†å®Œæˆ');
    }
  }, []);
  
  // æäº¤ç­”æ¡ˆ
  const handleSubmitAnswer = useCallback(async (answer: string) => {
    try {
      await submitAnswer(answer);
      
      // è·å–æœ€æ–°çš„é—®é¢˜å¹¶æ’­æ”¾
      if (voiceSettings.autoPlay) {
        // ç­‰å¾…storeæ›´æ–°
        setTimeout(() => {
          const latestQuestion = useInterviewStore.getState().currentQuestion;
          if (latestQuestion) {
            speakText(latestQuestion);
          }
        }, 500);
      }
    } catch (error) {
      console.error('æäº¤ç­”æ¡ˆå¤±è´¥:', error);
      toast.error('æäº¤ç­”æ¡ˆå¤±è´¥ï¼Œè¯·é‡è¯•');
    }
  }, [submitAnswer, voiceSettings.autoPlay]);
  
  // è¯­éŸ³åˆæˆ
  const speakText = useCallback(async (text: string) => {
    console.log('[RealTimeVoiceInterview:TTS] å¼€å§‹è¯­éŸ³åˆæˆï¼Œæ–‡æœ¬é•¿åº¦:', text.length);
    console.log('[RealTimeVoiceInterview:TTS] æ–‡æœ¬å†…å®¹:', text);
    
    if (voiceState.isSpeaking) {
      // å¦‚æœå…è®¸æ‰“æ–­ï¼Œåœæ­¢å½“å‰æ’­æ”¾
      if (voiceSettings.interruptible && audioRef.current) {
        console.log('[RealTimeVoiceInterview:TTS] æ‰“æ–­å½“å‰æ’­æ”¾...');
        audioRef.current.pause();
        audioRef.current.currentTime = 0;
      } else {
        console.log('[RealTimeVoiceInterview:TTS] å½“å‰æ­£åœ¨æ’­æ”¾ï¼Œä¸”ä¸å…è®¸æ‰“æ–­ï¼Œè·³è¿‡');
        return;
      }
    }
    
    setVoiceState(prev => ({ ...prev, isSpeaking: true }));
    
    try {
      console.log('[RealTimeVoiceInterview:TTS] å‘é€TTSè¯·æ±‚...');
      console.log('[RealTimeVoiceInterview:TTS] è¯·æ±‚å‚æ•°:', {
        voice: voiceSettings.voice,
        speed: voiceSettings.speed
      });
      
      const response = await fetch('http://localhost:8000/speech/synthesize', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/x-www-form-urlencoded',
        },
        body: new URLSearchParams({
          text,
          voice: voiceSettings.voice,
          speed: voiceSettings.speed.toString()
        })
      });
      
      console.log('[RealTimeVoiceInterview:TTS] TTSå“åº”çŠ¶æ€:', response.status);
      console.log('[RealTimeVoiceInterview:TTS] TTSå“åº”å¤´:', {
        contentType: response.headers.get('content-type'),
        contentLength: response.headers.get('content-length')
      });
      
      if (response.ok) {
        const audioBlob = await response.blob();
        console.log('[RealTimeVoiceInterview:TTS] éŸ³é¢‘Blobæ¥æ”¶æˆåŠŸï¼Œå¤§å°:', audioBlob.size, 'ç±»å‹:', audioBlob.type);
        
        const audioUrl = URL.createObjectURL(audioBlob);
        console.log('[RealTimeVoiceInterview:TTS] éŸ³é¢‘URLåˆ›å»ºæˆåŠŸ:', audioUrl);
        
        const interviewerMessage: VoiceMessage = {
          id: Date.now().toString(),
          role: 'interviewer',
          content: text,
          timestamp: new Date(),
          audioUrl,
          isPlaying: true
        };
        
        setMessages(prev => [...prev, interviewerMessage]);
        
        if (audioRef.current) {
          console.log('[RealTimeVoiceInterview:TTS] è®¾ç½®éŸ³é¢‘æº...');
          audioRef.current.src = audioUrl;
          
          // æ·»åŠ é”™è¯¯å¤„ç†
          audioRef.current.onerror = (e) => {
            console.error('[RealTimeVoiceInterview:TTS] éŸ³é¢‘æ’­æ”¾é”™è¯¯:', e);
            console.error('[RealTimeVoiceInterview:TTS] éŸ³é¢‘å…ƒç´ é”™è¯¯è¯¦æƒ…:', {
              error: audioRef.current?.error,
              errorCode: audioRef.current?.error?.code,
              errorMessage: audioRef.current?.error?.message
            });
            toast.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥');
            setVoiceState(prev => ({ ...prev, isSpeaking: false }));
          };
          
          // ç¡®ä¿éŸ³é¢‘å¯ä»¥æ’­æ”¾
          try {
            // è®¾ç½®éŸ³é‡
            audioRef.current.volume = 1.0;
            // ç¡®ä¿ä¸æ˜¯é™éŸ³
            audioRef.current.muted = false;
            
            console.log('[RealTimeVoiceInterview:TTS] å‡†å¤‡æ’­æ”¾éŸ³é¢‘...');
            const playPromise = audioRef.current.play();
            
            if (playPromise !== undefined) {
              playPromise
                .then(() => {
                  console.log('[RealTimeVoiceInterview:TTS] éŸ³é¢‘å¼€å§‹æ’­æ”¾æˆåŠŸ');
                })
                .catch((error) => {
                  console.error('[RealTimeVoiceInterview:TTS] æ’­æ”¾å¤±è´¥:', error);
                  console.error('[RealTimeVoiceInterview:TTS] æ’­æ”¾é”™è¯¯è¯¦æƒ…:', {
                    errorName: error.name,
                    errorMessage: error.message,
                    errorCode: error.code
                  });
                  // å¦‚æœæ˜¯è‡ªåŠ¨æ’­æ”¾ç­–ç•¥é—®é¢˜ï¼Œæç¤ºç”¨æˆ·
                  if (error.name === 'NotAllowedError') {
                    toast.error('è¯·ç‚¹å‡»é¡µé¢ä»»æ„ä½ç½®ä»¥å¯ç”¨éŸ³é¢‘æ’­æ”¾');
                    // æ·»åŠ ç‚¹å‡»äº‹ä»¶ç›‘å¬å™¨
                    const handleClick = async () => {
                      try {
                        await audioRef.current?.play();
                        console.log('[RealTimeVoiceInterview:TTS] ç”¨æˆ·ç‚¹å‡»åæ’­æ”¾æˆåŠŸ');
                        document.removeEventListener('click', handleClick);
                      } catch (e) {
                        console.error('[RealTimeVoiceInterview:TTS] ç‚¹å‡»åæ’­æ”¾ä»ç„¶å¤±è´¥:', e);
                      }
                    };
                    document.addEventListener('click', handleClick);
                  } else {
                    toast.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨è®¾ç½®');
                  }
                  setVoiceState(prev => ({ ...prev, isSpeaking: false }));
                });
            }
          } catch (playError) {
            console.error('[RealTimeVoiceInterview:TTS] æ’­æ”¾å¼‚å¸¸:', playError);
            toast.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥');
            setVoiceState(prev => ({ ...prev, isSpeaking: false }));
          }
        }
      } else {
        const errorText = await response.text();
        console.error('[RealTimeVoiceInterview:TTS] è¯­éŸ³åˆæˆå“åº”é”™è¯¯:', response.status, errorText);
        toast.error(`è¯­éŸ³åˆæˆå¤±è´¥: ${response.status}`);
        setVoiceState(prev => ({ ...prev, isSpeaking: false }));
      }
    } catch (error) {
      console.error('[RealTimeVoiceInterview:TTS] è¯­éŸ³åˆæˆå¤±è´¥:', error);
      console.error('[RealTimeVoiceInterview:TTS] é”™è¯¯è¯¦æƒ…:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      toast.error('è¯­éŸ³åˆæˆå¤±è´¥');
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
    }
  }, [voiceState.isSpeaking, voiceSettings]);
  
  // åˆ‡æ¢ç›‘å¬çŠ¶æ€
  const toggleListening = useCallback(() => {
    if (voiceState.isListening) {
      stopListening();
    } else {
      startListening();
    }
  }, [voiceState.isListening, startListening, stopListening]);
  
  // åˆå§‹åŒ–é¢è¯•
  useEffect(() => {
    if (sessionId && currentQuestion && messages.length === 0) {
      const welcomeMessage: VoiceMessage = {
        id: 'welcome',
        role: 'interviewer',
        content: `æ¬¢è¿å‚åŠ é¢è¯•ï¼${currentQuestion}`,
        timestamp: new Date()
      };
      
      setMessages([welcomeMessage]);
      
      // å»¶è¿Ÿæ’­æ”¾ï¼Œç¡®ä¿ç»„ä»¶å·²ç»å®Œå…¨åŠ è½½
      if (voiceSettings.autoPlay) {
        setTimeout(() => {
          speakText(welcomeMessage.content);
        }, 1000);
      }
    }
  }, [sessionId, currentQuestion, messages.length, voiceSettings.autoPlay]);
  
  // æ¸…ç†èµ„æº
  useEffect(() => {
    return () => {
      stopListening();
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, [stopListening]);
  
  // éŸ³é¢‘æ’­æ”¾ç»“æŸå¤„ç†
  const handleAudioEnded = useCallback(() => {
    console.log('éŸ³é¢‘æ’­æ”¾ç»“æŸ');
    setVoiceState(prev => ({ ...prev, isSpeaking: false }));
    setMessages(prev => prev.map(msg => ({ ...msg, isPlaying: false })));
    
    // è‡ªåŠ¨å¼€å§‹ç›‘å¬
    if (voiceSettings.autoPlay && !voiceState.isListening) {
      setTimeout(() => {
        startListening();
      }, 500);
    }
  }, [voiceSettings.autoPlay, voiceState.isListening, startListening]);
  
  // æ·»åŠ éŸ³é¢‘åŠ è½½æˆåŠŸå¤„ç†
  const handleAudioCanPlay = useCallback(() => {
    console.log('éŸ³é¢‘å¯ä»¥æ’­æ”¾äº†');
  }, []);
  
  // æ·»åŠ éŸ³é¢‘åŠ è½½å¼€å§‹å¤„ç†
  const handleAudioLoadStart = useCallback(() => {
    console.log('éŸ³é¢‘å¼€å§‹åŠ è½½');
  }, []);
  
  if (!sessionId) {
    return (
      <div className="flex items-center justify-center h-64">
        <div className="text-center">
          <div className="text-gray-500 mb-2">é¢è¯•ä¼šè¯æœªåˆå§‹åŒ–</div>
          <div className="text-sm text-gray-400">è¯·è¿”å›è®¾ç½®é¡µé¢é‡æ–°å¼€å§‹</div>
        </div>
      </div>
    );
  }
  
  return (
    <div className="max-w-4xl mx-auto p-6 h-screen flex flex-col">
      {/* å¤´éƒ¨çŠ¶æ€æ  */}
      <div className="bg-white rounded-lg shadow-sm p-4 mb-4">
        <div className="flex items-center justify-between">
          <div className="flex items-center space-x-4">
            <div className={`w-3 h-3 rounded-full ${
              voiceState.isConnected ? 'bg-green-500' : 'bg-red-500'
            }`}></div>
            <span className="text-sm font-medium">
              {voiceState.isListening ? 'æ­£åœ¨ç›‘å¬...' : 
               voiceState.isProcessing ? 'å¤„ç†ä¸­...' : 
               voiceState.isSpeaking ? 'é¢è¯•å®˜è¯´è¯ä¸­...' : 'å¾…æœºä¸­'}
            </span>
          </div>
          
          <div className="flex items-center space-x-4">
            {/* éŸ³é‡æŒ‡ç¤ºå™¨ */}
            <div className="flex items-center space-x-2">
              <span className="text-xs text-gray-500">éŸ³é‡</span>
              <div className="w-16 h-2 bg-gray-200 rounded-full overflow-hidden">
                <div 
                  className="h-full bg-blue-500 transition-all duration-100"
                  style={{ width: `${voiceState.volume * 100}%` }}
                ></div>
              </div>
            </div>
            
            {/* ç½®ä¿¡åº¦ */}
            {voiceState.confidence > 0 && (
              <div className="text-xs text-gray-500">
                ç½®ä¿¡åº¦: {(voiceState.confidence * 100).toFixed(1)}%
              </div>
            )}
          </div>
        </div>
      </div>
      
      {/* å¯¹è¯åŒºåŸŸ */}
      <div className="flex-1 bg-white rounded-lg shadow-sm p-4 overflow-y-auto mb-4">
        <div className="space-y-4">
          {messages.map((message) => (
            <div key={message.id} className={`flex ${
              message.role === 'interviewer' ? 'justify-start' : 'justify-end'
            }`}>
              <div className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
                message.role === 'interviewer' 
                  ? 'bg-blue-100 text-blue-900' 
                  : 'bg-gray-100 text-gray-900'
              }`}>
                <div className="flex items-start space-x-2">
                  <div className="flex-1">
                    <div className="text-sm font-medium mb-1">
                      {message.role === 'interviewer' ? 'ğŸ¤– é¢è¯•å®˜' : 'ğŸ‘¤ æ‚¨'}
                    </div>
                    <div className="text-sm">{message.content}</div>
                    {message.duration && (
                      <div className="text-xs text-gray-500 mt-1">
                        æ—¶é•¿: {message.duration.toFixed(1)}s
                      </div>
                    )}
                  </div>
                  
                  {message.audioUrl && (
                    <button
                      onClick={() => speakText(message.content)}
                      className="text-blue-500 hover:text-blue-700 p-1"
                      disabled={voiceState.isSpeaking}
                    >
                      {message.isPlaying ? 'ğŸ”Š' : 'ğŸ”‡'}
                    </button>
                  )}
                </div>
              </div>
            </div>
          ))}
          
          {voiceState.isProcessing && (
            <div className="flex justify-center">
              <div className="bg-gray-100 rounded-lg px-4 py-2">
                <div className="flex items-center space-x-2">
                  <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce"></div>
                  <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                  <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                  <span className="text-sm text-gray-600 ml-2">å¤„ç†è¯­éŸ³ä¸­...</span>
                </div>
              </div>
            </div>
          )}
        </div>
      </div>
      
      {/* æ§åˆ¶é¢æ¿ */}
      <div className="bg-white rounded-lg shadow-sm p-4">
        <div className="flex items-center justify-center space-x-4">
          {/* ä¸»è¦æ§åˆ¶æŒ‰é’® */}
          <button
            onClick={toggleListening}
            disabled={voiceState.isProcessing || isLoading}
            className={`w-16 h-16 rounded-full flex items-center justify-center text-white text-2xl transition-all duration-200 ${
              voiceState.isListening
                ? 'bg-red-500 hover:bg-red-600 animate-pulse'
                : 'bg-blue-500 hover:bg-blue-600'
            } disabled:opacity-50 disabled:cursor-not-allowed`}
          >
            {voiceState.isListening ? 'ğŸ›‘' : 'ğŸ¤'}
          </button>
          
          {/* æµ‹è¯•è¯­éŸ³æŒ‰é’® */}
          <button
            onClick={() => speakText('æµ‹è¯•è¯­éŸ³åŠŸèƒ½ï¼Œé¢è¯•å®˜æ­£åœ¨è¯´è¯')}
            disabled={voiceState.isSpeaking}
            className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600 disabled:opacity-50"
          >
            æµ‹è¯•è¯­éŸ³
          </button>
          
          {/* è°ƒè¯•æ¨¡å¼åˆ‡æ¢ */}
          <button
            onClick={() => setDebugMode(!debugMode)}
            className="px-4 py-2 bg-gray-500 text-white rounded hover:bg-gray-600"
          >
            {debugMode ? 'å…³é—­è°ƒè¯•' : 'å¼€å¯è°ƒè¯•'}
          </button>
          
          {/* è®¾ç½®æŒ‰é’® */}
          <div className="flex items-center space-x-2">
            <select
              value={voiceSettings.voice}
              onChange={(e) => setVoiceSettings(prev => ({ ...prev, voice: e.target.value }))}
              className="text-sm border rounded px-2 py-1"
              title="é€‰æ‹©é¢è¯•å®˜å£°éŸ³"
            >
              <option value="alloy">Alloy</option>
              <option value="echo">Echo</option>
              <option value="fable">Fable</option>
              <option value="onyx">Onyx</option>
              <option value="nova">Nova</option>
              <option value="shimmer">Shimmer</option>
            </select>
            
            <label className="flex items-center space-x-1 text-sm">
              <input
                type="checkbox"
                checked={voiceSettings.autoPlay}
                onChange={(e) => setVoiceSettings(prev => ({ ...prev, autoPlay: e.target.checked }))}
              />
              <span>è‡ªåŠ¨æ’­æ”¾</span>
            </label>
            
            <label className="flex items-center space-x-1 text-sm">
              <input
                type="checkbox"
                checked={voiceSettings.interruptible}
                onChange={(e) => setVoiceSettings(prev => ({ ...prev, interruptible: e.target.checked }))}
              />
              <span>å¯æ‰“æ–­</span>
            </label>
          </div>
        </div>
        
        {/* å½“å‰è¾“å…¥æ˜¾ç¤º */}
        {currentInput && (
          <div className="mt-4 p-3 bg-gray-50 rounded-lg">
            <div className="text-sm text-gray-600 mb-1">å½“å‰è¯†åˆ«ç»“æœ:</div>
            <div className="text-sm">{currentInput}</div>
          </div>
        )}
      </div>
      
      {/* éŸ³é¢‘æ’­æ”¾å™¨ - è°ƒè¯•æ¨¡å¼ä¸‹å¯è§ */}
      <audio
        ref={audioRef}
        onEnded={handleAudioEnded}
        onCanPlay={handleAudioCanPlay}
        onLoadStart={handleAudioLoadStart}
        onError={(e) => {
          console.error('éŸ³é¢‘å…ƒç´ é”™è¯¯:', e);
          const audio = e.currentTarget;
          console.error('é”™è¯¯è¯¦æƒ…:', {
            error: audio.error,
            src: audio.src,
            readyState: audio.readyState,
            networkState: audio.networkState
          });
        }}
        preload="auto"
        controls={debugMode}
        crossOrigin="anonymous"
        style={{ display: debugMode ? 'block' : 'none' }}
        className={debugMode ? 'mt-4 w-full' : ''}
      />
    </div>
  );
};

export default RealTimeVoiceInterview;